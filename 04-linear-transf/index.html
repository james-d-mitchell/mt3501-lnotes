<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
   "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="data:;base64,iVBORw0KGgo=" />
    <link rel="stylesheet" href="https://jdbm.me/mt3501-lnotes/css/math.css" />
    
    
    <title>MT3501 Lecture Notes | </title>
    <style type="text/css">
  body {
    font-size: 150%;
    font-family: muli,avenir,helvetica neue,helvetica,ubuntu,roboto,noto,segoe ui,arial,sans-serif;
  }
</style>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

</head>
<body><p><a name="nav-menu" id="nav-menu"><strong>Contents</strong></a></p>

<ul>
    
    <li>
      <a href="/mt3501-lnotes/">
      
      Home
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/problems/">
      
      Problems
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/01-intro/">
      
      Section 1 - Intro
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/02-vector-spaces/">
      
      Section 2 - Vector spaces
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/03-algorithms-to-live-by/">
      
      Section 3 - Algorithms to live by
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/04-linear-transf/">
      
      Section 4 - Linear transformations
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/05-l-v-w/">
      
      Section 5 - The vector space of linear transformations
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/06-direct-sums/">
      
      Section 6 - Direct sums
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/07-dual-space/">
      
      Section 7 - The dual space
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/08-eigen-stuff/">
      
      Section 8 - Eigenvectors, eigenvalues, and the characteristic polynomial
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/09-diagonal/">
      
      Section 9 - Diagonalisation
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/10-jnf/">
      
      Section 10 - Jordan normal form
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/11-inner-products/">
      
      Section 11 - Inner products
      </a>
    </li>
    
</ul>



    <script
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"
        type="text/javascript"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    






  </p>






<h1 id="linear-transformations">Linear transformations</h1>
<style type="text/css" scoped>
  body {
    counter-reset: chapter 3;
  }
</style>

<h2 id="definition-and-basic-properties">Definition and basic properties</h2>
<p>Linear transformations are functions between vector spaces that interact well with the vector space structure and probably the most important thing we study in linear algebra.</p>
<div class="defn">
<p><span id="def:lintransf" label="def:lintransf"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be vector spaces over the same field <span class="math inline">\(F\)</span>. A <strong><em>linear transformation</em></strong> from <span class="math inline">\(V\)</span> to <span class="math inline">\(W\)</span> is a function <span class="math inline">\(T : V \longrightarrow W\)</span> such that</p>
<ol type="1">
<li><p><span class="math inline">\(T(u+v) = T(u) + T(v)\)</span> for all <span class="math inline">\(u,v \in V\)</span>, and</p></li>
<li><p><span class="math inline">\(T(\alpha v) = \alpha T(v)\)</span> for all <span class="math inline">\(v \in V\)</span> and <span class="math inline">\(\alpha \in  F\)</span>.</p></li>
</ol>
</div>
<p>Linear transformations were discussed in great detail during the MT2501 module. We recall below some of these facts but omit the proofs in the lectures.</p>
<div class="lemma">
<p><span id="lem:lintrans" label="lem:lintrans"></span> Let <span class="math inline">\(T : V \longrightarrow W\)</span> be a linear transformation between two vector spaces over the field <span class="math inline">\(F\)</span>. Then</p>
<ol type="1">
<li><p>if <span class="math inline">\(v_{1},v_{2},\ldots,v_{k} \in V\)</span> and <span class="math inline">\(\alpha_{1},\alpha_{2},\ldots,\alpha_{k} \in F\)</span>, then <span class="math display">\[T\biggl( \sum_{i=1}^{k} \alpha_{i}v_{i} \biggr) = \sum_{i=1}^{k}
            \alpha_{i} T(v_{i}) .\]</span></p></li>
<li><p><span class="math inline">\(T(\vec{0}) = \vec{0}\)</span>;</p></li>
<li><p><span class="math inline">\(T(-v) = -T(v)\)</span> for all <span class="math inline">\(v \in V\)</span>;</p></li>
</ol>
</div>
<div class="prop">
<p><span id="prop:mapconstruct" label="prop:mapconstruct"></span> Let <span class="math inline">\(V\)</span> be a finite-dimensional vector space over the field <span class="math inline">\(F\)</span> with basis <span class="math inline">\(\mathscr{B} = \{ v_{1},v_{2},\ldots,v_{n} \}\)</span> and let <span class="math inline">\(W\)</span> be any vector space over <span class="math inline">\(F\)</span>. If <span class="math inline">\(y_{1}\)</span>, <span class="math inline">\(y_{2}\)</span>, …, <span class="math inline">\(y_{n}\in W\)</span> are arbitrary, then the <em>unique</em> linear transformation <span class="math inline">\(T : V \longrightarrow W\)</span> such that <span class="math inline">\(T(v_{i}) = y_{i}\)</span> for all <span class="math inline">\(i = 1\)</span>, <span class="math inline">\(2\)</span>, …, <span class="math inline">\(n\)</span> is defined by <span class="math display">\[T\left(\sum_{i = 1} ^ {n} \alpha_i v_i\right) =\sum_{i = 1} ^ {n} \alpha_i
    T(v_i).\]</span></p>
</div>
<div class="defn">
<p>Let <span class="math inline">\(T : V \longrightarrow W\)</span> be a linear transformation between vector spaces over a field <span class="math inline">\(F\)</span>.</p>
<ol type="1">
<li><p>The <strong><em>image</em></strong> of <span class="math inline">\(T\)</span> is <span class="math display">\[T(V) = \operatorname{im} T = \{T(v) : v \in V\}.\]</span></p></li>
<li><p>The <strong><em>kernel</em></strong> or <strong><em>null space</em></strong> of <span class="math inline">\(T\)</span> is <span class="math display">\[\ker T = \{v \in V : T(v) = \vec{0}_{W}\},\]</span> where <span class="math inline">\(\vec{0}_W\)</span> is the zero vector in <span class="math inline">\(W\)</span>.</p></li>
</ol>
</div>
<div class="prop">
<p><span id="prop-image-kernel-subspaces" label="prop-image-kernel-subspaces"></span> Let <span class="math inline">\(T : V \longrightarrow W\)</span> be a linear transformation between vector spaces <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> over the field <span class="math inline">\(F\)</span>. The image and kernel of <span class="math inline">\(T\)</span> are subspaces of <span class="math inline">\(W\)</span> and <span class="math inline">\(V\)</span>, respectively.</p>
</div>
<div class="defn">
<p>Let <span class="math inline">\(T : V \longrightarrow W\)</span> be a linear transformation between vector spaces over the field <span class="math inline">\(F\)</span>.</p>
<ol type="1">
<li><p>The <strong><em>rank</em></strong> of <span class="math inline">\(T\)</span>, which we shall denote <span class="math inline">\(\operatorname{rank}{T}\)</span>, is the dimension of the image of <span class="math inline">\(T\)</span>.</p></li>
<li><p>The <strong><em>nullity</em></strong> of <span class="math inline">\(T\)</span>, which we shall denote <span class="math inline">\(\operatorname{null}{T}\)</span>, is the dimension of the null space of <span class="math inline">\(T\)</span>.</p></li>
</ol>
</div>
<div class="thm">
<p><span id="thm-rank-nullity" label="thm-rank-nullity"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be vector spaces over the field <span class="math inline">\(F\)</span> with <span class="math inline">\(V\)</span> finite-dimensional and let <span class="math inline">\(T : V \longrightarrow W\)</span> be a linear transformation. Then <span class="math display">\[\operatorname{rank}{T} + \operatorname{null}{T} = \dim V.\]</span></p>
</div>
<p>For those who have taken MT2505, the Rank-Nullity Theorem can be viewed as an analogue of the First Isomorphism Theorem for groups within the world of vector spaces. Rearranging gives <span class="math display">\[\dim V - \dim \ker T = \dim \operatorname{im} T\]</span> and since (as we shall see later) dimension essentially determines vector spaces we conclude <span class="math display">\[V / \ker T \cong \operatorname{im} T.\]</span> Of course, we have not yet specified what is meant by a quotient or an isomorphism, but this does give some context for the theorem.</p>
<div class="omittedexampjupyter">
<p><span id="ex:linmap1" label="ex:linmap1"></span> Define a linear transformation <span class="math inline">\(T : \mathbb{R}^{4} \longrightarrow\mathbb{R}^{3}\)</span> in terms of the standard basis <span class="math inline">\(\mathscr{B} = \{ \vec{e}_{1},\vec{e}_{2},\vec{e}_{3},\vec{e}_{4} \}\)</span> by <span class="math display">\[\begin{aligned}
    T(\vec{e}_{1}) &amp; = \vec{y}_{1} = \begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix},
              &amp; T(\vec{e}_{2})                             &amp; = \vec{y}_{2} = \begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix},   \\
    T(\vec{e}_{3}) &amp; = \vec{y}_{3} = \begin{pmatrix} 0 \\ 1 \\ 5 \\ \end{pmatrix},
              &amp; T(\vec{e}_{4})                             &amp; = \vec{y}_{4} = \begin{pmatrix} -5 \\ -2 \\ -5 \\ \end{pmatrix}.
  \end{aligned}\]</span> Calculate the linear transformation <span class="math inline">\(T\)</span> and its rank and nullity.</p>
</div>
<div class="solution">
<p>The effect of <span class="math inline">\(T\)</span> on an arbitrary vector of <span class="math inline">\(\mathbb{R}^{4}\)</span> can be calculated by the linearity property: <span class="math display">\[\begin{aligned}
    T \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ \delta \end{pmatrix}
     &amp; = T( \alpha\vec{e}_{1} + \beta\vec{e}_{2} + \gamma\vec{e}_{3} + \delta\vec{e}_{4} )    \\
     &amp; = \alpha T(\vec{e}_{1}) + \beta T(\vec{e}_{2}) + \gamma T(\vec{e}_{3}) + \delta
    T(\vec{e}_{4})                                                             \\
     &amp; = \alpha\begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix} + \beta\begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix} +
    \gamma\begin{pmatrix} 0 \\ 1 \\ 5 \\ \end{pmatrix} + \delta\begin{pmatrix} -5 \\ -2 \\ -5 \\ \end{pmatrix}              \\
     &amp; = \begin{pmatrix} 2\alpha-\beta-5\delta \\ \alpha+\gamma-2\delta \\ 3\alpha
     +\beta+5\gamma-5\delta \end{pmatrix}.
  \end{aligned}\]</span></p>
<p>[<span class="smallcaps">Exercise:</span> Check by hand that this formula does really define a linear transformation <span class="math inline">\(T : \mathbb{R}^{4} \longrightarrow\mathbb{R}^{3}\)</span>.]</p>
<p>Now let us determine the kernel of this transformation <span class="math inline">\(T\)</span>. Suppose <span class="math inline">\(\vec{v} \in \ker T\)</span>. Here <span class="math inline">\(\vec{v}\)</span> is some vector in <span class="math inline">\(\mathbb{R}^{4}\)</span>, say <span class="math display">\[\vec{v} = \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ \delta \end{pmatrix} =
    \alpha\vec{e}_{1} + \beta\vec{e}_{2} + \gamma\vec{e}_{3} + \delta\vec{e}_{4}\]</span> where <span class="math display">\[T(\vec{v}) =
    \begin{pmatrix} 2\alpha-\beta-5\delta \\ 
      \alpha+\gamma-2\delta \\ 
      3\alpha +\beta+5\gamma-5\delta
    \end{pmatrix} =
    \begin{pmatrix} 0 \\ 0 \\ 0 \\ \end{pmatrix}.\]</span> We have here three simultaneous equations in four variables which we convert to the matrix equation <span class="math display">\[\begin{pmatrix}
      2 &amp; -1 &amp; 0 &amp; -5 \\
      1 &amp; 0  &amp; 1 &amp; -2 \\
      3 &amp; 1  &amp; 5 &amp; -5
    \end{pmatrix}
    \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ \delta \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \end{pmatrix}.\]</span> We solve this by performing the usual row operations used in Gaussian elimination [see MT2501]: <span class="math display">\[\begin{aligned}
    \left( \begin{array}{c|c}
        \begin{matrix}
          2 &amp; -1 &amp; 0 &amp; -5 \\
          1 &amp; 0  &amp; 1 &amp; -2 \\
          3 &amp; 1  &amp; 5 &amp; -5
        \end{matrix} &amp;
        \begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
      \end{array} \right) &amp; \longrightarrow
    %
    \left( \begin{array}{c|c}
        \begin{matrix}
          1 &amp; 0  &amp; 1 &amp; -2 \\
          2 &amp; -1 &amp; 0 &amp; -5 \\
          3 &amp; 1  &amp; 5 &amp; -5
        \end{matrix} &amp;
        \begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
      \end{array} \right)
                                               &amp;                 &amp; (r_{1} \leftrightarrow r_{2})     \\
    %
                                               &amp; \longrightarrow
    \left( \begin{array}{c|c}
        \begin{matrix}
          1 &amp; 0  &amp; 1  &amp; -2 \\
          0 &amp; -1 &amp; -2 &amp; -1 \\
          0 &amp; 1  &amp; 2  &amp; 1
        \end{matrix} &amp;
        \begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
      \end{array} \right)
                                               &amp;                 &amp; \!\!\!\begin{array}{r@{}l}
      ( r_{2} &amp; {} \mapsto r_{2}-2r_{1}, \\
      r_{3}   &amp; {} \mapsto r_{3}-3r_{1})
    \end{array} \\
    %
                                               &amp; \longrightarrow
    \left( \begin{array}{c|c}
        \begin{matrix}
          1 &amp; 0 &amp; 1 &amp; -2 \\
          0 &amp; 1 &amp; 2 &amp; 1  \\
          0 &amp; 0 &amp; 0 &amp; 0
        \end{matrix} &amp;
        \begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
      \end{array} \right)
                                               &amp;                 &amp; \!\!\!\begin{array}{r@{}l}
      ( r_{3} &amp; {} \mapsto r_{3} + r_{2}, \\
      r_{2}   &amp; {} \mapsto -r_{2} )
    \end{array}
  \end{aligned}\]</span> So given arbitrary <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\delta\)</span>, we require <span class="math display">\[\alpha + \gamma -2\delta = 0
    \qquad \text{and} \qquad
    \beta + 2\gamma + \delta = 0.\]</span> We remain with two degrees of freedom (the free choice of <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\delta\)</span>) and so <span class="math inline">\(\ker T\)</span> is <span class="math inline">\(2\)</span>-dimensional: <span class="math display">\[\begin{aligned}
    \ker T &amp; = \left\{
    \begin{pmatrix} -\gamma+2\delta \\ -2\gamma-\delta \\ \gamma \\ \delta \end{pmatrix}
    \;\middle|\; \gamma,\delta \in \mathbb{R} \right\}                                    \\
           &amp; = \left\{ \gamma\begin{pmatrix} -1 \\ -2 \\ 1 \\ 0 \end{pmatrix} + \delta\begin{pmatrix} 2 \\ -1 \\ 0 \\ 1 \end{pmatrix}
    \;\middle|\; \gamma,\delta \in \mathbb{R} \right\}                                    \\
           &amp; = \operatorname{Span} \left( \begin{pmatrix} -1 \\ -2 \\ 1 \\ 0 \end{pmatrix},
    \begin{pmatrix} 2 \\ -1 \\ 0 \\ 1 \end{pmatrix} \right).
  \end{aligned}\]</span> It is easy to check these two spanning vectors are linearly independent, so <span class="math inline">\(\operatorname{null}{T} = \dim \ker T = 2\)</span>. The Rank-Nullity Theorem then says <span class="math display">\[\operatorname{rank}{T} = \dim \mathbb{R}^{4} - \operatorname{null}{T} = 4 - 2 = 2.\]</span> Essentially this boils down to the four image vectors <span class="math inline">\(\vec{y}_{1}\)</span>, <span class="math inline">\(\vec{y}_{2}\)</span>, <span class="math inline">\(\vec{y}_{3}\)</span>, <span class="math inline">\(\vec{y}_{4}\)</span> spanning a <span class="math inline">\(2\)</span>-dimensional space. Indeed, note that they are not linearly independent because <span class="math display">\[\begin{gathered}
    \vec{y}_{3} = \begin{pmatrix} 0 \\ 1 \\ 5 \\ \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix} + 2
    \begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix} = \vec{y}_{1} + 2\vec{y}_{2} \\
    \vec{y}_{4} = \begin{pmatrix} -5 \\ -2 \\ -5 \\ \end{pmatrix} = -2\begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix} +
    \begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix} = -2\vec{y}_{1} + \vec{y}_{2}.
  \end{gathered}\]</span> The full explanation behind this lies in the following result.</p>
</div>
<p>We have described the basic facts about linear transformations. It is possible to describe various examples of linear transformations, some of which can seem natural, some more esoteric.</p>
<div class="prop">
<p><span id="prop-surj-inj" label="prop-surj-inj"></span> Let <span class="math inline">\(V\)</span> be a finite-dimensional vector space over the field <span class="math inline">\(F\)</span> with basis <span class="math inline">\(\{ v_{1},v_{2},\ldots,v_{n} \}\)</span> and let <span class="math inline">\(W\)</span> be a vector space over <span class="math inline">\(F\)</span>. Fix vectors <span class="math inline">\(y_{1}\)</span>, <span class="math inline">\(y_{2}\)</span>, …, <span class="math inline">\(y_{n}\)</span> in <span class="math inline">\(W\)</span> and let <span class="math inline">\(T : V \longrightarrow W\)</span> be the unique linear transformation given by <span class="math inline">\(T(v_{i}) = y_{i}\)</span> for <span class="math inline">\(i = 1\)</span>, <span class="math inline">\(2\)</span>, …, <span class="math inline">\(n\)</span>. Then</p>
<ol type="1">
<li><p><span class="math inline">\(\operatorname{im} T = \operatorname{Span}(y_{1},y_{2},\ldots,y_{n})\)</span>.</p></li>
<li><p><span class="math inline">\(\ker T = \{\vec{0}\}\)</span> if and only if <span class="math inline">\(\{y_{1},y_{2},\ldots,y_{n}\}\)</span> is a linearly independent set.</p></li>
</ol>
</div>
<div class="proof">
<p><em>Proof.</em> <strong>(1)</strong> If <span class="math inline">\(x \in \operatorname{im} T\)</span>, then <span class="math inline">\(x = T(v)\)</span> for some <span class="math inline">\(v \in V\)</span>. We can write <span class="math inline">\(v = \sum_{i=1}^{n} \alpha_{i}v_{i}\)</span> for some <span class="math inline">\(\alpha_{i} \in  F\)</span>. Then <span class="math display">\[x = T(v) = T\biggl( \sum_{i=1}^{n} \alpha_{i}v_{i} \biggr) =
    \sum_{i=1}^{n} \alpha_{i} T(v_{i}) = \sum_{i=1}^{n} \alpha_{i}y_{i} \in
    \operatorname{Span}(y_1, y_2, \ldots, y_n).\]</span> Hence <span class="math inline">\(\operatorname{im} T \subseteq \operatorname{Span}(y_1, y_2, \ldots,  y_n)\)</span>. On the other hand, since <span class="math inline">\(y_i = T(v_i)\in \operatorname{im} T\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(\operatorname{im} T\)</span> is a subspace of <span class="math inline">\(W\)</span> (<a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-image-kernel-subspaces">Proposition 4.1.5</a>), it follows that <span class="math inline">\(\operatorname{Span}(y_1, y_2, \ldots, y_n) \subseteq \operatorname{im} T\)</span>. Thus <span class="math inline">\(\operatorname{im} T = \operatorname{Span}(y_1, y_2, \ldots, y_n)\)</span>.</p>
<p><strong>(2)</strong> (<span class="math inline">\(\Rightarrow\)</span>) Suppose that <span class="math inline">\(\ker T = \{\vec{0}\}\)</span>. By part (1), <span class="math inline">\(\operatorname{im} T = \operatorname{Span}(y_{1},y_{2},\ldots,y_{n})\)</span> and so by <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#thm-basis-subset">Corollary 2.4.10</a>, <span class="math inline">\(\{y_1, y_2, \ldots, y_n\}\)</span> contains a linearly independent subset <span class="math inline">\(\mathscr{B}\)</span> such that <span class="math inline">\(\operatorname{Span}(\mathscr{B})  = \operatorname{Span}(y_1, y_2, \ldots, y_n) = \operatorname{im} T\)</span>. It follows that <span class="math inline">\(|\mathscr{B}| = \dim \operatorname{im} T = \operatorname{rank}  T\)</span>. By the Rank-Nullity Theorem, <span class="math inline">\(\dim V = n = \operatorname{rank} T +  \operatorname{null} T = \operatorname{rank} T = \dim \operatorname{im} T\)</span>, since <span class="math inline">\(\operatorname{null} T = \dim \ker T = 0\)</span>. Therefore <span class="math inline">\(\mathscr{B} =  \{y_1, y_2, \ldots, y_n\}\)</span> and so <span class="math inline">\(\{y_1, y_2, \ldots, y_n\}\)</span> is a basis for <span class="math inline">\(\operatorname{im} T\)</span> and, in particular, it is linearly independent.</p>
<p>(<span class="math inline">\(\Leftarrow\)</span>) If <span class="math inline">\(\{y_{1},y_{2},\ldots,y_{n}\}\)</span> is linearly independent, then since <span class="math inline">\(\operatorname{Span}(y_{1},y_{2},\ldots,y_{n}) = \operatorname{im} T\)</span>, it follows that <span class="math inline">\(\{y_{1},y_{2},\ldots,y_{n}\}\)</span> is a basis for <span class="math inline">\(\operatorname{im} T\)</span> (<a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#thm-3-conditions">Theorem 2.4.8</a>) and so <span class="math inline">\(\operatorname{rank} T = n\)</span>. Hence, by the Rank-Nullity Theorem, <span class="math inline">\(\dim \ker T = \operatorname{null} T = \dim V - \operatorname{rank} V = n - n = 0\)</span>, and so <span class="math inline">\(\ker T = \{\vec{0}\}\)</span>.</p>
<p><strong>Alternative direct proof of (2).</strong> (<span class="math inline">\(\Leftarrow\)</span>) Suppose that <span class="math inline">\(\{y_1, y_2, \ldots, y_n\}\)</span> is linearly independent. If <span class="math inline">\(v\in  \ker T\)</span> is arbitrary, then <span class="math inline">\(v = \sum_{i=1}^{n} \alpha_{i}v_{i}\)</span> for some <span class="math inline">\(\alpha_i\in F\)</span> and <span class="math display">\[\vec{0} = T(v) = \sum_{i=1}^{n} \alpha_{i}T(v_{i}) =
    \sum_{i=1}^{n} \alpha_{i}y_{i}.\]</span> Since <span class="math inline">\(\{y_1, y_2, \ldots, y_n\}\)</span> is linearly independent, it follows that <span class="math inline">\(\alpha_i = 0\)</span> for all <span class="math inline">\(i\)</span> and so <span class="math inline">\(v = \vec{0}\)</span>.</p>
<p>(<span class="math inline">\(\Rightarrow\)</span>) Suppose that <span class="math inline">\(\ker T = \{\vec{0}\}\)</span>. If <span class="math inline">\(\sum_{i=1}^{n} \alpha_{i}y_{i} = \vec{0}\in W\)</span> for some <span class="math inline">\(\alpha_i\in F\)</span>, then <span class="math display">\[T\left(\sum_{i=1}^{n} \alpha_{i}v_{i}\right) = \sum_{i=1}^{n}
    \alpha_{i}T(v_{i}) = \sum_{i=1}^{n} \alpha_{i}y_{i} = \vec{0},\]</span> and so <span class="math inline">\(\sum_{i=1}^{n} \alpha_{i}v_{i}\in \ker T = \{\vec{0}\}\)</span>. It follows that <span class="math inline">\(\sum_{i=1}^{n} \alpha_{i}v_{i} = \vec{0}\)</span> and so <span class="math inline">\(\alpha_i = 0\)</span> for all <span class="math inline">\(i\)</span>, and so <span class="math inline">\(\{y_1, y_2, \ldots, y_n\}\)</span> is linearly independent. ◻</p>
</div>
<div class="omittedexamp">
<p>Define a linear transformation <span class="math inline">\(T : \mathbb{R}^{3} \longrightarrow\mathbb{R}^{3}\)</span> in terms of the standard basis <span class="math inline">\(\mathscr{B} = \{ \vec{e}_{1},\vec{e}_{2},\vec{e}_{3} \}\)</span> by <span class="math display">\[T(\vec{e}_{1}) = \vec{y}_{1} = \begin{pmatrix} 2 \\ 1 \\ -1 \\ \end{pmatrix}, \quad
    T(\vec{e}_{2}) = \vec{y}_{2} = \begin{pmatrix} -1 \\ 0 \\ 2 \\ \end{pmatrix}, \quad
    T(\vec{e}_{3}) = \vec{y}_{3} = \begin{pmatrix} 0 \\ -1 \\ 4 \\ \end{pmatrix}.\]</span> Show that <span class="math inline">\(\ker T = \{\vec{0}\}\)</span> and <span class="math inline">\(\operatorname{im} T = \mathbb{R}^{3}\)</span>.</p>
</div>
<div class="solution">
<p>We check whether <span class="math inline">\(\{ \vec{y}_{1}, \vec{y}_{2}, \vec{y}_{3} \}\)</span> is linearly independent. Solve <span class="math display">\[\alpha \vec{y}_{1} + \beta \vec{y}_{2} + \gamma \vec{y}_{3} = \vec{0};\]</span> that is, <span class="math display">\[\begin{aligned}
    2\alpha \;\, - \beta \,\qquad &amp; = 0  \\
    \alpha \;\;\,\qquad -\gamma   &amp; = 0  \\
    -\alpha + 2\beta + 4\gamma    &amp; = 0.
  \end{aligned}\]</span> The second equation tells us that <span class="math inline">\(\gamma = \alpha\)</span> while the first says <span class="math inline">\(\beta = 2\alpha\)</span>. Substituting for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> in the third equation gives <span class="math display">\[-\alpha + 4\alpha + 4\alpha = 7\alpha = 0.\]</span> Hence <span class="math inline">\(\alpha = 0\)</span> and consequently <span class="math inline">\(\beta = \gamma = 0\)</span>.</p>
<p>This shows <span class="math inline">\(\{ \vec{y}_{1}, \vec{y}_{2}, \vec{y}_{3} \}\)</span> is linearly independent. Consequently, <span class="math inline">\(\ker T = \{\vec{0}\}\)</span> by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a>. The Rank-Nullity Theorem now says <span class="math display">\[\dim \operatorname{im} T = \dim \mathbb{R}^{3} - \dim \ker T = 3 - 0 = 3.\]</span> Therefore <span class="math inline">\(\operatorname{im} T = \mathbb{R}^{3}\)</span> as it has the same dimension.</p>
<p>[Alternatively, since <span class="math inline">\(\dim \mathbb{R}^{3} = 3\)</span> and <span class="math inline">\(\{ \vec{y}_{1},  \vec{y}_{2}, \vec{y}_{3} \}\)</span> is linearly independent, this set must be a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span> (see <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#thm-3-conditions">Theorem 2.4.8</a>). Therefore, by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a>(1), <span class="math display">\[\operatorname{im} T =
        \operatorname{Span}(\vec{y}_{1},\vec{y}_{2},\vec{y}_{3}) =
      \mathbb{R}^{3},\]</span> once again.]</p>
</div>
<div class="omittedexamp">
<p>Let <span class="math inline">\(T : \mathbb{R}^{4} \longrightarrow\mathbb{R}^{3}\)</span> be the linear transformation defined in terms of the standard basis <span class="math inline">\(\mathscr{B} = \{ \vec{e}_{1}, \vec{e}_{2},  \vec{e}_{3}, \vec{e}_{4} \}\)</span> by <span class="math display">\[\begin{aligned}
    T(\vec{e}_{1}) &amp; = \vec{y}_{1} = \begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix}, &amp; T(\vec{e}_{2}) &amp; =
    \vec{y}_{2} = \begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix}                               \\
    T(\vec{e}_{3}) &amp; = \vec{y}_{3} = \begin{pmatrix} 0 \\ 1 \\ 5 \\ \end{pmatrix}, &amp; T(\vec{e}_{4}) &amp; =
    \vec{y}_{4} = \begin{pmatrix} -5 \\ -2 \\ -5 \\ \end{pmatrix}.
  \end{aligned}\]</span> Find a basis for the image of <span class="math inline">\(T\)</span>.</p>
</div>
<div class="solution">
<p>This is the linear transformation considered in <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#ex:linmap1">Example 4.1A</a>. We observed there that <span class="math inline">\(\dim \operatorname{im} T =  \operatorname{rank} T = 2\)</span>. We also know from <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a> that <span class="math display">\[\operatorname{im} T = \operatorname{Span}( \vec{y}_{1}, \vec{y}_{2}, \vec{y}_{3},
    \vec{y}_{4} ),\]</span> so we conclude that <span class="math inline">\(\operatorname{im} T\)</span> has a basis <span class="math inline">\(\mathscr{C}\)</span> containing <span class="math inline">\(2\)</span> vectors and satisfying <span class="math inline">\(\mathscr{C} \subseteq \{ \vec{y}_{1},  \vec{y}_{2}, \vec{y}_{3}, \vec{y}_{4} \}\)</span>. Note that <span class="math display">\[\{ \vec{y}_{1}, \vec{y}_{2} \} = \left\{ \begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix},
    \begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix} \right\}\]</span> is linearly independent. Indeed if <span class="math display">\[\alpha \begin{pmatrix} 2 \\ 1 \\ 3 \\ \end{pmatrix} + \beta \begin{pmatrix} -1 \\ 0 \\ 1 \\ \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \end{pmatrix}\]</span> then we deduce straight away <span class="math inline">\(\alpha = 0\)</span> and then <span class="math inline">\(\beta = 0\)</span>. We now have a linearly independent subset of <span class="math inline">\(\operatorname{im} T\)</span> of the right size to be a basis. Hence <span class="math inline">\(\mathscr{C} = \{ \vec{y}_{1}, \vec{y}_{2}  \}\)</span> is a basis for <span class="math inline">\(\operatorname{im} T\)</span>.</p>
</div>
<h2 id="the-matrix-of-a-linear-transformation">The matrix of a linear transformation</h2>
<p>We have attempted to describe an arbitrary linear transformation <span class="math inline">\(T: V \longrightarrow W\)</span>. Given a basis <span class="math inline">\(\{ v_{1},v_{2},\ldots,v_{n} \}\)</span> for <span class="math inline">\(V\)</span>, we have observed that <span class="math inline">\(T\)</span> is uniquely determined by specifying the images <span class="math inline">\(T(v_{1})\)</span>, <span class="math inline">\(T(v_{2})\)</span>, …, <span class="math inline">\(T(v_{n})\)</span> of the basis vectors. If we are also given a basis for <span class="math inline">\(W\)</span>, we can then express these image vectors as a linear combination of the basis vectors of <span class="math inline">\(W\)</span> and hence completely specify them.</p>
<div class="defn">
<p>Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite-dimensional vector spaces over the field <span class="math inline">\(F\)</span> and let <span class="math inline">\(\mathscr{B} = \{ v_{1},v_{2},\ldots,v_{n} \}\)</span> and <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2},\ldots,w_{m} \}\)</span> be bases for <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, respectively. If <span class="math inline">\(T : V \longrightarrow W\)</span> is a linear transformation and <span class="math inline">\(v_j \in \mathscr{B}\)</span>, then <span class="math inline">\(T(v_j)\in W\)</span> and so <span class="math inline">\(T(v_j)\)</span> is a linear combination of vectors in <span class="math inline">\(\mathscr{C}\)</span>: <span class="math display">\[T(v_{j}) = \sum_{i=1}^{m} \alpha_{ij} w_{i}\]</span> for <span class="math inline">\(j = 1\)</span>, <span class="math inline">\(2\)</span>, …, <span class="math inline">\(n\)</span>. The <span class="math inline">\(m \times n\)</span> matrix <span class="math display">\[=
    \begin{pmatrix}
      \alpha_{11} &amp; \alpha_{12} &amp; \cdots &amp; \alpha_{1n} \\
      \alpha_{21} &amp; \alpha_{22} &amp; \cdots &amp; \alpha_{2n} \\
      \vdots      &amp; \vdots      &amp; \ddots &amp; \vdots      \\
      \alpha_{m1} &amp; \alpha_{m2} &amp; \cdots &amp; \alpha_{mn}
    \end{pmatrix}\]</span> is called the <strong><em>matrix of <span class="math inline">\(T\)</span> with respect to the bases <span class="math inline">\(\mathscr{B}\)</span> and <span class="math inline">\(\mathscr{C}\)</span></em></strong>. We shall denote this by <span class="math inline">\(\operatorname{Mat}(T)\)</span> or, to be more explicit <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{C}}(T)\)</span>.</p>
<p>In other words, the entries of the <span class="math inline">\(j\)</span>th column of the matrix <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{C}}(T)\)</span> are: <span class="math display">\[\begin{matrix}
      \alpha_{1j} \\ \alpha_{2j} \\ \vdots \\ \alpha_{mj}
    \end{matrix}\]</span> i.e., the <span class="math inline">\(j\)</span>th column specifies the image of <span class="math inline">\(T(v_{j})\)</span> by listing the coefficients when it is expressed as a linear combination of the vectors in <span class="math inline">\(\mathscr{C}\)</span>.</p>
</div>
<h5 id="what-does-the-matrix-of-a-linear-transformation-actually-represent">What does the matrix of a linear transformation actually represent?</h5>
<p>This question could be answered at great length and can get as complicated and subtle as one wants. The short answer is that if <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>-dimensional vector spaces over a field <span class="math inline">\(F\)</span>, then they “look like” <span class="math inline">\(F^{m}\)</span> and <span class="math inline">\(F^{n}\)</span> (formally, are <em>isomorphic</em> to these spaces, see below for details). Then <span class="math inline">\(T\)</span> maps vectors from <span class="math inline">\(V\)</span> into <span class="math inline">\(W\)</span> in the same way that the matrix <span class="math inline">\(\operatorname{Mat}(T)\)</span> maps vectors from <span class="math inline">\(F^{m}\)</span> into <span class="math inline">\(F^{n}\)</span>. (There is a technical formulation of what “in the same way” means here, but that goes way beyond the requirements of this course. It will result in the kernels of the two linear maps being of the same dimension, similarly for the images, etc.)</p>
<div class="exampjupyter">
<p><span id="ex:lintrans-matrix" label="ex:lintrans-matrix"></span> Define a linear transformation <span class="math inline">\(T : \mathbb{R}^{4} \longrightarrow\mathbb{R}^{4}\)</span> by the following formula: <span class="math display">\[T \begin{pmatrix} x \\
 y \\
 z \\
 t \\
 \end{pmatrix} = \begin{pmatrix} x+4y \\
 y \\
 2z+t \\
 z+2t \\
 \end{pmatrix}.\]</span> Let <span class="math inline">\(\mathscr{B} = \{ \vec{e}_{1},\vec{e}_{2},\vec{e}_{3},\vec{e}_{4} \}\)</span> denote the standard basis for <span class="math inline">\(\mathbb{R}^{4}\)</span> and let <span class="math inline">\(\mathscr{C}\)</span> be the basis <span class="math display">\[\mathscr{C} = \{ \vec{v}_{1},\vec{v}_{2},\vec{v}_{3},\vec{v}_{4}
    \} = \left\{ \begin{pmatrix} 2 \\
 0 \\
 2 \\
 0 \\
 \end{pmatrix}, \begin{pmatrix} 0 \\
 1 \\
 -1 \\
 0 \\
 \end{pmatrix},
    \begin{pmatrix} 0 \\
 0 \\
 1 \\
 0 \\
 \end{pmatrix}, \begin{pmatrix} 3 \\
 0 \\
 0 \\
 1 \\
 \end{pmatrix} \right\}.\]</span> Determine the matrices <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span>,  <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{B}}(T)\)</span> and <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T)\)</span>.</p>
</div>
<div class="solution">
<p>We calculate <span class="math display">\[\begin{aligned}
    T(\vec{e}_{1}) &amp; = T\begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} = \vec{e}_{1}
    \\
    T(\vec{e}_{2}) &amp; = T\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \\ 0 \\ 0 \end{pmatrix} =
    4\vec{e}_{1} + \vec{e}_{2}                                                    \\
    T(\vec{e}_{3}) &amp; = T\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 2 \\ 1 \end{pmatrix} =
    2\vec{e}_{3} + \vec{e}_{4}                                                    \\
    T(\vec{e}_{4}) &amp; = T\begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 2 \end{pmatrix} = \vec{e}_{3}
    + 2\vec{e}_{4}.
  \end{aligned}\]</span> So the matrix of <span class="math inline">\(T\)</span> with respect to the basis <span class="math inline">\(\mathscr{B}\)</span> is <span class="math display">\[\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T) = \begin{pmatrix}
      1 &amp; 4 &amp; 0 &amp; 0 \\
      0 &amp; 1 &amp; 0 &amp; 0 \\
      0 &amp; 0 &amp; 2 &amp; 1 \\
      0 &amp; 0 &amp; 1 &amp; 2
    \end{pmatrix}.\]</span></p>
<p>[We leave it as an exercise for the reader to check that <span class="math inline">\(\mathscr{C}\)</span> is indeed a basis for <span class="math inline">\(\mathbb{R}^{4}\)</span>. Do this by showing it is linearly independent, i.e., the only solution to <span class="math display">\[\begin{pmatrix}
      2 &amp; 0  &amp; 0 &amp; 3 \\
      0 &amp; 1  &amp; 0 &amp; 0 \\
      2 &amp; -1 &amp; 1 &amp; 0 \\
      0 &amp; 0  &amp; 0 &amp; 1
    \end{pmatrix}
    \begin{pmatrix} \alpha \\ \beta \\ \gamma \\ \delta \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}\]</span> is <span class="math inline">\(\alpha = \beta = \gamma = \delta = 0\)</span>.]</p>
<p>We shall calculate the matrices <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{B}}(T)\)</span> and <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T)\)</span>. <span class="math display">\[\begin{aligned}
    T(\vec{v}_{1}) &amp; = T\begin{pmatrix} 2 \\ 0 \\ 2 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 0 \\ 4 \\ 2 \end{pmatrix} =
    2\vec{e}_{1} + 4\vec{e}_{3} + 2\vec{e}_{4}                                         \\
    T(\vec{v}_{2}) &amp; = T\begin{pmatrix} 0 \\ 1 \\ -1 \\ 0 \end{pmatrix} = \begin{pmatrix} 4 \\ 1 \\ -2 \\ -1 \end{pmatrix}
    = 4\vec{e}_{1} + \vec{e}_{2} - 2\vec{e}_{3} - \vec{e}_{4}                               \\
    T(\vec{v}_{3}) &amp; = T\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 2 \\ 1 \end{pmatrix} =
    2\vec{e}_{3} + \vec{e}_{4}                                                    \\
    T(\vec{v}_{4}) &amp; = T\begin{pmatrix} 3 \\ 0 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 3 \\ 0 \\ 1 \\ 2 \end{pmatrix} =
    3\vec{e}_{1} + \vec{e}_{3} + 2\vec{e}_{4}.
  \end{aligned}\]</span> Hence <span class="math display">\[\operatorname{Mat}_{\mathscr{C},\mathscr{B}}(T) = \begin{pmatrix}
      2 &amp; 4  &amp; 0 &amp; 3 \\
      0 &amp; 1  &amp; 0 &amp; 0 \\
      4 &amp; -2 &amp; 2 &amp; 1 \\
      2 &amp; -1 &amp; 1 &amp; 2
    \end{pmatrix}.\]</span></p>
<p>To find <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T)\)</span>, we need to express each <span class="math inline">\(T(\vec{v}_{j})\)</span> in terms of the basis <span class="math inline">\(\mathscr{C}\)</span>. <span class="math display">\[\begin{aligned}
    T(\vec{v}_{1}) = \begin{pmatrix} 2 \\ 0 \\ 4 \\ 2 \end{pmatrix}   &amp; = -2\begin{pmatrix} 2 \\ 0 \\ 2 \\ 0 \end{pmatrix} +
    8\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} + 2\begin{pmatrix} 3 \\ 0 \\ 0 \\ 1 \end{pmatrix}                                                       \\
                                             &amp; = -2\vec{v}_{1} + 8\vec{v}_{3} + 2\vec{v}_{4}         \\
    T(\vec{v}_{2}) = \begin{pmatrix} 4 \\ 1 \\ -2 \\ -1 \end{pmatrix} &amp; =
    \frac{7}{2}\begin{pmatrix} 2 \\ 0 \\ 2 \\ 0 \end{pmatrix} + \begin{pmatrix} 0 \\ 1 \\ -1 \\ 0 \end{pmatrix} -
    8\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} - \begin{pmatrix} 3 \\ 0 \\ 0 \\ 1 \end{pmatrix}                                                        \\
                                             &amp; = {\textstyle\frac{7}{2}}\vec{v}_{1} + \vec{v}_{2} -
    8\vec{v}_{3} - \vec{v}_{4}                                                                        \\
    T(\vec{v}_{3}) = \begin{pmatrix} 0 \\ 0 \\ 2 \\ 1 \end{pmatrix}   &amp; =
    -\frac{3}{2}\begin{pmatrix} 2 \\ 0 \\ 2 \\ 0 \end{pmatrix} + 5 \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} +
    \begin{pmatrix} 3 \\ 0 \\ 0 \\ 1 \end{pmatrix}                                                                                \\
                                             &amp; = {\textstyle-\frac{3}{2}}\vec{v}_{1} + 5\vec{v}_{3} +
    \vec{v}_{4}                                                                                        \\
    T(\vec{v}_{4}) = \begin{pmatrix} 3 \\ 0 \\ 1 \\ 2 \end{pmatrix}   
                 &amp; = -\frac{3}{2}\begin{pmatrix} 2 \\ 0 \\ 2 \\ 0 \end{pmatrix}
                     + 4\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} +
                       2\begin{pmatrix} 3 \\ 0 \\ 0 \\ 1 \end{pmatrix} \\
                 &amp; = {\textstyle-\frac{3}{2}}\vec{v}_{1} + 4\vec{v}_{3} +
                 2\vec{v}_{4}.
  \end{aligned}\]</span> Hence <span class="math display">\[\operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T) = 
    \frac{1}{2}
    \begin{pmatrix}
      -4 &amp; 7   &amp; -3 &amp; -3 \\
      0  &amp; 2   &amp; 0  &amp; 0  \\
      16 &amp; -16 &amp; 10 &amp; 8  \\
      4  &amp; -2  &amp; 2  &amp; 4
    \end{pmatrix}.\]</span></p>
</div>
<p>We show in Problem <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#problem-04-06">6</a> in <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#problems-04-linear-transf">Section 4.5</a> that if <span class="math inline">\(A\)</span> is an <span class="math inline">\(m\times n\)</span> matrix with entries in a field <span class="math inline">\(F\)</span> and <span class="math inline">\(T: F ^ n \longrightarrow F ^ m\)</span> is the function defined by <span class="math inline">\(T(\vec{v}) = A\vec{v}\)</span>, then <span class="math inline">\(T\)</span> is a linear transformation.</p>
<div class="thm">
<p><span id="thm-column-space-is-image" label="thm-column-space-is-image"></span> Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(m\times n\)</span> matrix with entries in a field <span class="math inline">\(F\)</span>. If <span class="math inline">\(T: F ^ n  \longrightarrow F ^ m\)</span> is the linear transformation defined by <span class="math inline">\(T(\vec{v}) = A\vec{v}\)</span>, then the following hold:</p>
<ol type="1">
<li><p>the image of <span class="math inline">\(T\)</span> equals the column-space of <span class="math inline">\(A\)</span>, i.e. <span class="math inline">\(\operatorname{im}(T) =  \operatorname{Col}(A)\)</span>;</p></li>
<li><p>the rank of <span class="math inline">\(T\)</span> equals the column-rank of <span class="math inline">\(A\)</span>, i.e. <span class="math inline">\(\operatorname{rank}(T) = \dim  \operatorname{Col}(A)\)</span>.</p></li>
</ol>
</div>
<p>It is not the case that the row-space of <span class="math inline">\(A\)</span> in <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-column-space-is-image">Theorem 4.2.3</a> is <span class="math inline">\(\ker(T)\)</span>, although <span class="math inline">\(\operatorname{Row}(A)\)</span> and <span class="math inline">\(\ker(T)\)</span> are related.</p>
<h2 id="change-of-basis">Change of basis</h2>
<p>Suppose we are given two bases <span class="math inline">\(\mathscr{B}\)</span> and <span class="math inline">\(\mathscr{C}\)</span> for the same vector space <span class="math inline">\(V\)</span>. We shall now describe how <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span> and <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T)\)</span> are related for some linear transformation <span class="math inline">\(T : V \longrightarrow V\)</span>. (A similar description can be given for a linear transformation <span class="math inline">\(V \longrightarrow W\)</span> with two bases <span class="math inline">\(\mathscr{B},\mathscr{B}&#39;\)</span> for <span class="math inline">\(V\)</span> and two bases <span class="math inline">\(\mathscr{C},\mathscr{C}&#39;\)</span> for <span class="math inline">\(W\)</span>. This would be more complicated, but essentially the same ideas apply.)</p>
<div class="thm">
<p><span id="thm-change-basis" label="thm-change-basis"></span> Let <span class="math inline">\(V\)</span> be a vector space of dimension <span class="math inline">\(n\)</span> over a field <span class="math inline">\(F\)</span> and let <span class="math inline">\(T : V \longrightarrow V\)</span> be a linear transformation, let <span class="math inline">\(\mathscr{B} = \{  v_{1},v_{2},\ldots,v_{n} \}\)</span> and <span class="math inline">\(\mathscr{C} = \{  w_{1},w_{2},\ldots,w_{n} \}\)</span> be bases for <span class="math inline">\(V\)</span>, and let <span class="math inline">\(A = \operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span> and <span class="math inline">\(B = \operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T)\)</span>. Then there is an invertible matrix <span class="math inline">\(P\)</span> such that <span class="math display">\[B = P^{-1} A P = \operatorname{Mat}_{\mathscr{B},
    \mathscr{C}}(\operatorname{id})\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)
  \operatorname{Mat}_{\mathscr{C}, \mathscr{B}}(\operatorname{id})\]</span> where <span class="math inline">\(\operatorname{id}: V \longrightarrow V\)</span> is the linear transformation defined by <span class="math inline">\(\operatorname{id}(v) = v\)</span> for all <span class="math inline">\(v\in V\)</span>. Specifically, if <span class="math inline">\(w_j\in  \mathscr{C}\)</span>, then <span class="math inline">\(w_j\in V = \operatorname{Span}(\mathscr{B})\)</span> and so <span class="math inline">\(w_j\)</span> is a linear combination of vectors in <span class="math inline">\(\mathscr{B}\)</span>: <span class="math display">\[w_{j} = \operatorname{id}(w_j) = \sum_{i=1}^{n} \alpha_{ij}v_{i}\]</span> for some <span class="math inline">\(\alpha_{ij} \in F\)</span>, and <span class="math display">\[P = [\alpha_{kj}] =
    \begin{pmatrix}
      \alpha_{11} &amp; \alpha_{12} &amp; \cdots &amp; \alpha_{1n} \\
      \alpha_{21} &amp; \alpha_{22} &amp; \cdots &amp; \alpha_{2n} \\
      \vdots      &amp; \vdots      &amp; \ddots &amp; \vdots      \\
      \alpha_{n1} &amp; \alpha_{n2} &amp; \cdots &amp; \alpha_{nn}
    \end{pmatrix}
    =
    \operatorname{Mat}_{\mathscr{C}, \mathscr{B}}(\operatorname{id}).\]</span></p>
</div>
<p>The more general version of <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-change-basis">Theorem 4.3.1</a> is the next result which is included just for interest.</p>
<div class="thm">
<p><span id="thm-general-base-change" label="thm-general-base-change"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite-dimensional vector spaces over a field <span class="math inline">\(F\)</span> and let <span class="math inline">\(T: V \longrightarrow W\)</span> be a linear transformation. If <span class="math inline">\(\mathscr{B}\)</span> and <span class="math inline">\(\mathscr{B}&#39;\)</span> are bases for <span class="math inline">\(V\)</span> and <span class="math inline">\(\mathscr{C}\)</span> and <span class="math inline">\(\mathscr{C}&#39;\)</span> are basis for <span class="math inline">\(W\)</span>, then <span class="math display">\[\operatorname{Mat}_{\mathscr{B}&#39;, \mathscr{C}&#39;}(T) =
  \operatorname{Mat}_{\mathscr{C}, \mathscr{C}&#39;}(\operatorname{id}_W)
  \operatorname{Mat}_{\mathscr{B}, \mathscr{C}}(T)
  \operatorname{Mat}_{\mathscr{B}&#39;, \mathscr{B}}(\operatorname{id}_V),\]</span> where <span class="math inline">\(\operatorname{id}_V : V \longrightarrow V\)</span> and <span class="math inline">\(\operatorname{id}_W: W \longrightarrow W\)</span> are the identity linear transformations.</p>
</div>
<p>If <span class="math inline">\(\dim V = n\)</span> and <span class="math inline">\(\dim W = m\)</span> in <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-general-base-change">Theorem 4.3.2</a>, then <span class="math inline">\(\operatorname{Mat}_{\mathscr{C}, \mathscr{C}&#39;}(\operatorname{id}_W)\)</span> is an <span class="math inline">\(m\times m\)</span> matrix, <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{C}}(T)\)</span> is an <span class="math inline">\(m\times n\)</span> matrix, and <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}&#39;}(\operatorname{id}_V)\)</span> is an <span class="math inline">\(n \times n\)</span> matrix, and so <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}&#39;, \mathscr{C}&#39;}(T)\)</span> is an <span class="math inline">\(m \times n\)</span> matrix.</p>
<p>Let us illustrate <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-change-basis">Theorem 4.3.1</a> with an example.</p>
<div class="exampjupyter">
<p>Let <span class="math inline">\(V\)</span> be a <span class="math inline">\(2\)</span>-dimensional vector space over <span class="math inline">\(\mathbb{R}\)</span> with basis <span class="math inline">\(\mathscr{B} = \{ v_{1},v_{2} \}\)</span>. Let <span class="math display">\[w_{1} = 3v_{1} - 5v_{2}, \qquad w_{2} = -v_{1}+2v_{2}\]</span> and <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2} \}\)</span>. Define the linear transformation <span class="math inline">\(T : V \longrightarrow V\)</span> by <span class="math display">\[\begin{aligned}
    T(v_{1}) &amp; = 16v_{1} - 30v_{2} \\
    T(v_{2}) &amp; = 9v_{1} - 17v_{2}.
  \end{aligned}\]</span> Find the matrix <span class="math inline">\(\operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T)\)</span>.</p>
</div>
<div class="solution">
<p>The formula for <span class="math inline">\(T\)</span> tells us that the matrix of <span class="math inline">\(T\)</span> in terms of the basis <span class="math inline">\(\mathscr{B}\)</span> is <span class="math display">\[A = \operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T) = 
    \begin{pmatrix} 
      16 &amp; 9 \\ 
      -30 &amp; -17
    \end{pmatrix}.\]</span> The formula in the example expresses the <span class="math inline">\(w_{j}\)</span> in terms of the <span class="math inline">\(v_{i}\)</span>. Hence, the change of basis matrix is <span class="math display">\[P = \operatorname{Mat}_{\mathscr{C}, \mathscr{B}}(\operatorname{id}) = \begin{pmatrix} 3 &amp; -1 \\ -5 &amp; 2 \end{pmatrix} .\]</span> Then <span class="math display">\[\det P = 3 \times 2 - (-1 \times -5) = 6 - 5 = 1,\]</span> so <span class="math display">\[P^{-1} = \frac{1}{\det P} \begin{pmatrix} 2 &amp; 1 \\ 5 &amp;
      3\end{pmatrix} = \begin{pmatrix} 2 &amp; 1 \\ 5 &amp; 3 \end{pmatrix} .\]</span> So <span class="math display">\[\begin{aligned}
    \operatorname{Mat}_{\mathscr{C},\mathscr{C}}(T) &amp; = P^{-1} A P                                                                          \\
                                     &amp; = \begin{pmatrix} 2 &amp; 1 \\ 5 &amp; 3 \end{pmatrix} \begin{pmatrix} 16
       &amp; 9 \\ -30 &amp; -17\end{pmatrix} \begin{pmatrix} 3 &amp; -1 \\ -5 &amp;
      2\end{pmatrix} \\
                                     &amp; = \begin{pmatrix} 2 &amp; 1 \\ -10 &amp; -6 \end{pmatrix} \begin{pmatrix}
      3 &amp; -1 \\ -5 &amp; 2 \end{pmatrix}                             \\
                                     &amp; = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -2 \end{pmatrix} .
  \end{aligned}\]</span> We have diagonalised our linear transformation <span class="math inline">\(T\)</span>. We shall discuss this topic in more detail later in these notes.</p>
<p>As a check, observe <span class="math display">\[\begin{aligned}
    T(w_{2}) &amp; = T( -v_{1}+2v_{2})                      \\
             &amp; = -T(v_{1}) + 2T(v_{2})                  \\
             &amp; = -(16v_{1}-30v_{2}) + 2(9v_{1}-17v_{2}) \\
             &amp; = 2v_{1} - 4v_{2}                        \\
             &amp; = -2 (-v_{1}+2v_{2}) = -2w_{2},
  \end{aligned}\]</span> and similarly for <span class="math inline">\(T(w_{1})\)</span>.</p>
</div>
<div class="omittedexampjupyter">
<p>Let <span class="math display">\[\mathscr{B} = 
    \left\{ 
      \begin{pmatrix} 0 \\ 1 \\ -1 \\ \end{pmatrix}, 
      \begin{pmatrix} 1 \\ 0 \\ -1 \\ \end{pmatrix},
      \begin{pmatrix} 2 \\ -1 \\ 0 \\ \end{pmatrix} 
    \right\}.\]</span></p>
<ol type="1">
<li><p>Show that <span class="math inline">\(\mathscr{B}\)</span> is a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p></li>
<li><p>Write down the change of basis matrix from the standard basis <span class="math inline">\(\mathscr{E} = \{ \vec{e}_{1},\vec{e}_{2},\vec{e}_{3} \}\)</span> to <span class="math inline">\(\mathscr{B}\)</span>.</p></li>
<li><p>Let <span class="math display">\[A = \begin{pmatrix}
              -2 &amp; -2 &amp; -3 \\
              1  &amp; 1  &amp; 2  \\
              -1 &amp; -2 &amp; -2
            \end{pmatrix}\]</span> and view <span class="math inline">\(A\)</span> as a linear transformation <span class="math inline">\(\mathbb{R}^{3} \longrightarrow  \mathbb{R}^{3}\)</span>. Find the matrix of <span class="math inline">\(A\)</span> with respect to the basis <span class="math inline">\(\mathscr{B}\)</span>.</p></li>
</ol>
</div>
<div class="solution">
<p>(1) We first establish that <span class="math inline">\(\mathscr{B}\)</span> is linearly independent. Solve <span class="math display">\[\alpha \begin{pmatrix} 0 \\ 1 \\ -1 \\ \end{pmatrix} + \beta \begin{pmatrix} 1 \\ 0 \\ -1 \\ \end{pmatrix} + \gamma
    \begin{pmatrix} 2 \\ -1 \\ 0 \\ \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \end{pmatrix};\]</span> that is, <span class="math display">\[\begin{aligned}
    \beta + 2\gamma         &amp; = 0 \\
    \alpha \:\qquad -\gamma &amp; = 0 \\
    -\alpha -\beta \qquad\; &amp; =0.
  \end{aligned}\]</span> Thus <span class="math inline">\(\gamma = \alpha\)</span> and the first equation yields <span class="math inline">\(2\alpha+\beta  = 0\)</span>. Adding the third equation now gives <span class="math inline">\(\alpha = 0\)</span> and hence <span class="math inline">\(\beta = \gamma = 0\)</span>. This show <span class="math inline">\(\mathscr{B}\)</span> is linearly independent and it is therefore a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span> since <span class="math inline">\(\dim \mathbb{R}^{3} = 3 =  |\mathscr{B}|\)</span>.</p>
<p>(2) We write each vector in <span class="math inline">\(\mathscr{B}\)</span> in terms of the standard basis <span class="math display">\[\begin{aligned}
    \begin{pmatrix} 0 \\ 1 \\ -1 \\ \end{pmatrix} &amp; = \qquad \vec{e}_{2} - \vec{e}_{3}   \\
    \begin{pmatrix} 1 \\ 0 \\ -1 \\ \end{pmatrix} &amp; = \; \vec{e}_{1} \qquad -\vec{e}_{3} \\
    \begin{pmatrix} 2 \\ -1 \\ 0 \\ \end{pmatrix} &amp; = 2\vec{e}_{1} - \vec{e}_{2}
  \end{aligned}\]</span> and write the coefficients appearing down the columns of the change of basis matrix: <span class="math display">\[P = \begin{pmatrix}
      0  &amp; 1  &amp; 2  \\
      1  &amp; 0  &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{pmatrix}.\]</span></p>
<p>(3) <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-change-basis">Theorem 4.3.1</a> says <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(A) =  P^{-1}AP\)</span> (as the matrix of <span class="math inline">\(A\)</span> with respect to the standard basis is <span class="math inline">\(A\)</span> itself). We first calculate the inverse of <span class="math inline">\(P\)</span> via the usual row operation method: <span class="math display">\[\begin{aligned}
    \left( \begin{matrix}
        0  &amp; 1  &amp; 2  \\
        1  &amp; 0  &amp; -1 \\
        -1 &amp; -1 &amp; 0\end{matrix} \;
    \middle| \; \begin{matrix}
        1 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 0 \\
        0 &amp; 0 &amp; 1\end{matrix}
    \right) &amp; \longrightarrow
    \left( \begin{matrix}
        0 &amp; 1  &amp; 2  \\
        1 &amp; 0  &amp; -1 \\
        0 &amp; -1 &amp; -1\end{matrix}
    \; \middle| \; \begin{matrix}
        1 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 0 \\
        0 &amp; 1 &amp; 1\end{matrix} \right)
            &amp;                 &amp; r_{3} \mapsto r_{3}+r_{1}   \\
            &amp; \longrightarrow
    \left( \begin{matrix}
        1 &amp; 0  &amp; -1 \\
        0 &amp; 1  &amp; 2  \\
        0 &amp; -1 &amp; -1\end{matrix}
    \; \middle| \; \begin{matrix}
        0 &amp; 1 &amp; 0 \\
        1 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 1\end{matrix} \right)
            &amp;                 &amp; r_{1} \leftrightarrow r_{2} \\
            &amp; \longrightarrow
    \left( \begin{matrix}
        1 &amp; 0 &amp; -1 \\
        0 &amp; 1 &amp; 2  \\
        0 &amp; 0 &amp; 1\end{matrix}
    \; \middle| \; \begin{matrix}
        0 &amp; 1 &amp; 0 \\
        1 &amp; 0 &amp; 0 \\
        1 &amp; 1 &amp; 1\end{matrix} \right)
            &amp;                 &amp; r_{3} \mapsto r_{3} + r_{2} \\
            &amp; \longrightarrow
    \left( \begin{matrix}
        1 &amp; 0 &amp; 0 \\
        0 &amp; 1 &amp; 0 \\
        0 &amp; 0 &amp; 1\end{matrix}
    \; \middle| \; \begin{matrix}
        1  &amp; 2  &amp; 1  \\
        -1 &amp; -2 &amp; -2 \\
        1  &amp; 1  &amp; 1\end{matrix} \right)
            &amp;                 &amp; \begin{array}{@{}l}
      r_{1} \mapsto r_{1} + r_{3} \\
      r_{2} \mapsto r_{2} - 2r_{3}
    \end{array}
  \end{aligned}\]</span> Hence <span class="math display">\[P^{-1} = \begin{pmatrix}
      1  &amp; 2  &amp; 1  \\
      -1 &amp; -2 &amp; -2 \\
      1  &amp; 1  &amp; 1
    \end{pmatrix}\]</span> and so <span class="math display">\[\begin{aligned}
    \operatorname{Mat}_{\mathscr{B},\mathscr{B}}(A) &amp; = P^{-1} A P \\
                   &amp; =
    \begin{pmatrix}
      1  &amp; 2  &amp; 1  \\
      -1 &amp; -2 &amp; -2 \\
      1  &amp; 1  &amp; 1
    \end{pmatrix}
    \begin{pmatrix}
      -2 &amp; -2 &amp; -3 \\
      1  &amp; 1  &amp; 2  \\
      -1 &amp; -2 &amp; -2
    \end{pmatrix}
    \begin{pmatrix}
      0  &amp; 1  &amp; 2  \\
      1  &amp; 0  &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{pmatrix}   \\
                   &amp; =
    \begin{pmatrix}
      -1 &amp; -2 &amp; -1 \\
      2  &amp; 4  &amp; 3  \\
      -2 &amp; -3 &amp; -3
    \end{pmatrix}
    \begin{pmatrix}
      0  &amp; 1  &amp; 2  \\
      1  &amp; 0  &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{pmatrix}   \\
                   &amp; =
    \begin{pmatrix}
      -1 &amp; 0  &amp; 0  \\
      1  &amp; -1 &amp; 0  \\
      0  &amp; 1  &amp; -1
    \end{pmatrix}.\square
  \end{aligned}\]</span></p>
</div>
<h2 id="isomorphism-of-vector-spaces">Isomorphism of vector spaces</h2>
<p>This section is not really revision of material covered in MT2501, but its most natural place in the course is in this chapter, so this is where it will appear.</p>
<p>If <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are vector spaces, then recall that a linear transformation <span class="math inline">\(T:  V \longrightarrow W\)</span> is <em>invertible</em> if there exists a linear transformation <span class="math inline">\(T ^  {-1} : W \longrightarrow V\)</span> such that <span class="math inline">\(T ^ {-1} \circ T(v) = v\)</span> and <span class="math inline">\(T \circ T ^ {-1}(w) =  w\)</span> for all <span class="math inline">\(v\in V\)</span> and for all <span class="math inline">\(w\in W\)</span>. The linear transformation <span class="math inline">\(T ^  {-1} : W \longrightarrow V\)</span> is called the <em>inverse</em> of <span class="math inline">\(T\)</span>.</p>
<div class="defn">
<p><span id="defn-isomorphism" label="defn-isomorphism"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be vector spaces over a field <span class="math inline">\(F\)</span>. An <strong><em>isomorphism</em></strong> between <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> is a linear transformation <span class="math inline">\(T : V \longrightarrow W\)</span> which is invertible. We say that <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are <strong><em>isomorphic</em></strong>, written <span class="math inline">\(V \cong W\)</span>, if there exists an isomorphism <span class="math inline">\(V \longrightarrow W\)</span>.</p>
</div>
<p>Recall that a function <span class="math inline">\(f: X \longrightarrow Y\)</span> is a <strong><em>bijection</em></strong> if it is injective (<span class="math inline">\(f(x) = f(y)\)</span> implies <span class="math inline">\(x = y\)</span>) and surjective (for every <span class="math inline">\(y\in Y\)</span> there exists <span class="math inline">\(x\in X\)</span> such that <span class="math inline">\(f(x) = y\)</span>). Bijections are sometimes called <strong><em>1-1 correspondences</em></strong>.</p>
<div class="lemma">
<p><span id="lemma-bijection" label="lemma-bijection"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be vector spaces over a field <span class="math inline">\(F\)</span>, and let <span class="math inline">\(T: V \longrightarrow W\)</span> be a linear transformation. Then <span class="math inline">\(T\)</span> is an isomorphism if and only if it is a bijection.</p>
</div>
<div class="proof">
<p><em>Proof.</em> See Problem <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#problem-04-03">3</a>(a) and (b) in <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#problems-04-linear-transf">Section 4.5</a>. ◻</p>
</div>
<div class="thm">
<p><span id="thm-bijection-basis" label="thm-bijection-basis"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite-dimensional vector spaces over a field <span class="math inline">\(F\)</span> and let <span class="math inline">\(\mathscr{B} = \{v_1, \ldots, v_n\}\)</span> be any basis for <span class="math inline">\(V\)</span>. If <span class="math inline">\(T: V \longrightarrow W\)</span> is an isomorphism, then <span class="math inline">\(T(\mathscr{B}) = \{T(v_1), \ldots, T(v_n)\}\)</span> is a basis for <span class="math inline">\(W\)</span>. Conversely, if <span class="math inline">\(\mathscr{C} = \{w_1, \ldots, w_n\}\)</span> is any basis for <span class="math inline">\(W\)</span>, then the unique linear transformation <span class="math inline">\(T: V \longrightarrow W\)</span> such that <span class="math inline">\(T(v_i) = w_i\)</span> for every <span class="math inline">\(i\)</span> is an isomorphism.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Let <span class="math inline">\(T: V \longrightarrow W\)</span> be any isomorphism from <span class="math inline">\(V\)</span> to <span class="math inline">\(W\)</span>. Since <span class="math inline">\(T\)</span> is a bijection (<a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#lemma-bijection">Lemma 4.4.2</a>), it follows that <span class="math inline">\(\operatorname{ker}(T) =  \{\vec{0}\}\)</span> (<a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a>). Hence, by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a>(2), <span class="math inline">\(T(\mathscr{B}) = \{T(v_1), \ldots, T(v_n)\}\)</span> is a linearly independent set. Also, by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a>(1) and since <span class="math inline">\(T\)</span> is a bijection, <span class="math inline">\(W = \operatorname{im} T = \operatorname{Span}(T(v_1), \ldots,  T(v_n))\)</span>. Therefore <span class="math inline">\(T(\mathscr{B})\)</span> is a linearly independent spanning set for <span class="math inline">\(W\)</span>, and so <span class="math inline">\(T(\mathscr{B})\)</span> is a basis for <span class="math inline">\(W\)</span>.</p>
<p>The converse is shown in Problem <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#problem-04-03">3</a>(c). ◻</p>
</div>
<div class="cor">
<p><span id="thm-isomorphism" label="thm-isomorphism"></span> Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite-dimensional vector spaces over a field <span class="math inline">\(F\)</span>. Then <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are isomorphic if and only if <span class="math inline">\(\dim V = \dim W\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> (<span class="math inline">\(\Rightarrow\)</span>) If <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are isomorphic, then there is an isomorphism <span class="math inline">\(T: V \longrightarrow W\)</span>. If <span class="math inline">\(\mathscr{B}\)</span> is any basis for <span class="math inline">\(V\)</span>, then <span class="math inline">\(T(\mathscr{B})\)</span> is a basis for <span class="math inline">\(W\)</span> by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-bijection-basis">Theorem 4.4.3</a>. Since <span class="math inline">\(T\)</span> is a bijection, <span class="math inline">\(\dim W =  |T(\mathscr{B})| = |\mathscr{B}| = \dim V\)</span>.</p>
<p>(<span class="math inline">\(\Leftarrow\)</span>) If <span class="math inline">\(\mathscr{B} = \{v_1, \ldots, v_m\}\)</span> is any basis for <span class="math inline">\(V\)</span> and <span class="math inline">\(\mathscr{C}  = \{w_1, \ldots, w_n\}\)</span> is any basis for <span class="math inline">\(W\)</span>, then by assumption <span class="math inline">\(m = \dim V  = \dim W = n\)</span>. So, by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-bijection-basis">Theorem 4.4.3</a>, the unique linear transformation <span class="math inline">\(T : V\longrightarrow W\)</span> such that <span class="math inline">\(T(v_i) = w_i\)</span> for every <span class="math inline">\(i\)</span> is an isomorphism. ◻</p>
</div>
<div class="cor">
<p><span id="cor-f-to-the-n" label="cor-f-to-the-n"></span> Let <span class="math inline">\(V\)</span> be a vector space over a field <span class="math inline">\(F\)</span>. If <span class="math inline">\(\dim V = n\in \mathbb{N}\)</span>, then <span class="math inline">\(V\)</span> is isomorphic to <span class="math inline">\(F ^ n\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> By <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-isomorphism">Corollary 4.4.4</a>, <span class="math inline">\(V\)</span> is isomorphic to any vector space over <span class="math inline">\(F\)</span> of dimension <span class="math inline">\(\dim V = n\)</span>. We showed in <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#ex-f-to-the-n">Example 2.1.4</a> that <span class="math inline">\(F ^ n\)</span> is a vector space over <span class="math inline">\(F\)</span> and, in <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#ex-standard-basis">Example 2.4.5</a>, that <span class="math inline">\(\dim F ^ n = n\)</span>. ◻</p>
</div>
<div class="example">
<p>We showed in <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#ex-f-to-the-n">Example 2.1.4</a>(1) that the set <span class="math inline">\(\mathcal{P}_n\)</span>, of polynomials with real coefficients and degree at most <span class="math inline">\(n\in \mathbb{N}\)</span>, <span class="math inline">\(n \geqslant 0\)</span>, is a vector space over <span class="math inline">\(\mathbb{R}\)</span> and, in <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#ex-standard-basis">Example 2.4.5</a>(2), that <span class="math inline">\(\dim \mathcal{P}_n = n + 1\)</span>. It follows by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#cor-f-to-the-n">Corollary 4.4.5</a> that <span class="math inline">\(\mathcal{P}_n \cong \mathbb{R} ^ {n + 1}\)</span>.</p>
</div>
<div class="example">
<p>We showed in <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#ex-f-to-the-n">Example 2.1.4</a>(2) that the set <span class="math inline">\(\mathbb{C}\)</span> is a vector space over <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(\dim \mathbb{C} = 2\)</span>. It follows by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#cor-f-to-the-n">Corollary 4.4.5</a> that <span class="math inline">\(\mathbb{C}\cong \mathbb{R} ^ {2}\)</span>.</p>
</div>
<h2 id="problems-04-linear-transf">Problems</h2>
<p>Problems marked with a 💻 (if any) can probably be solved more easily using a Jupyter notebook: <a href="https://moody.st-andrews.ac.uk/moodle/mod/lti/view.php?id=806990" class="uri">https://moody.st-andrews.ac.uk/moodle/mod/lti/view.php?id=806990</a></p>
<ol type="1">
<li><p><span id="problem-04-01" label="problem-04-01"></span></p>
<div class="questionjupyter">
<p>Define a function <span class="math inline">\(T \colon \mathbb{R}^{3} \longrightarrow\mathbb{R}^{3}\)</span> by <span class="math display">\[T \begin{pmatrix} x \\ y \\ z \end{pmatrix} 
      = \begin{pmatrix} x+3y-z \\ x+2y-2z \\ -x+4z \end{pmatrix}.\]</span></p>
<ol type="1">
<li><p>Show that <span class="math inline">\(T\)</span> is a linear transformation.</p></li>
<li><p>Determine the kernel of <span class="math inline">\(T\)</span> and find a basis for <span class="math inline">\(\ker T\)</span>. (Hint: Solving <span class="math inline">\(T(\vec{v}) = \vec{0}\)</span> will correspond to solving a set of simultaneous linear equations.)</p></li>
<li><p>Show that <span class="math display">\[\operatorname{im} T = \operatorname{Span} \left( \begin{pmatrix} 1 \\
              1 \\
              -1 \\
            \end{pmatrix},
            \begin{pmatrix} 3 \\
              2 \\
              0 \\
              \end{pmatrix}, \begin{pmatrix} -1 \\
              -2 \\
              4 \\
          \end{pmatrix} \right).\]</span> Hence find a basis for <span class="math inline">\(\operatorname{im} T\)</span>.</p></li>
<li><p>Verify the Rank-Nullity Theorem holds for this specific example.</p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p><span class="math display">\[\begin{aligned}
T \left( \begin{pmatrix}x_{1}\\y_{1}\\z_{1}\end{pmatrix} +
  \begin{pmatrix}x_{2}\\y_{2}\\z_{2}\end{pmatrix} \right) &amp;= T
\begin{pmatrix}x_{1}+x_{2}\\y_{1}+y_{2}\\z_{1}+z_{2}\end{pmatrix} 
= \begin{pmatrix} (x_{1}+x_{2}) + 3(y_{1}+y_{2}) - (z_{1}+z_{2}) \\
  (x_{1}+x_{2}) + 2(y_{1}+y_{2}) - 2(z_{1}+z_{2}) \\ -(x_{1}+x_{2}) +
  4(z_{1}+z_{2}) \end{pmatrix} \\
&amp;= \begin{pmatrix} (x_{1}+3y_{1}-z_{1}) + (x_{2}+3y_{2}-z_{2}) \\
  (x_{1}+2y_{1}-2z_{1}) + (x_{2}+2y_{2}-2z_{2}) \\ (-x_{1}+4z_{1}) +
  (-x_{2}+4z_{2}) \end{pmatrix} \\
&amp;= \begin{pmatrix} x_{1}+3y_{1}-z_{1} \\ x_{1}+2y_{1}-2z_{1} \\
  -x_{1}+4z_{1} \end{pmatrix} + \begin{pmatrix} x_{2}+3y_{2}-z_{2} \\
  x_{2}+2y_{2}-2z_{2} \\ -x_{2}+4z_{2} \end{pmatrix} 
= T\begin{pmatrix}x_{1}\\y_{1}\\z_{1}\end{pmatrix} + T\begin{pmatrix}x_{2}\\y_{2}\\z_{2}\end{pmatrix}\end{aligned}\]</span> and <span class="math display">\[\begin{aligned}
T \left( \alpha\begin{pmatrix}x\\y\\z\end{pmatrix} \right) &amp;= T \begin{pmatrix}\alpha
  x\\\alpha y\\\alpha z\end{pmatrix} 
= \begin{pmatrix} \alpha x + 3\alpha y - \alpha z \\ \alpha x + 2\alpha y
  - 2\alpha z \\ -\alpha x + 4\alpha z \end{pmatrix} 
= \alpha \begin{pmatrix}x+3y-z\\x+2y-2z\\-x+4z\end{pmatrix} 
= \alpha \; T\!\begin{pmatrix}x\\y\\z\end{pmatrix}.\end{aligned}\]</span> Hence <span class="math inline">\(T\)</span> is a linear transformation.</p>
<div class="center">
<hr />
</div></li>
<li><p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong></p>
<p>We solve <span class="math display">\[T\begin{pmatrix}x\\y\\z\end{pmatrix} = \begin{pmatrix}x+3y-z\\x+2y-2z\\-x+4z\end{pmatrix} =
\begin{pmatrix}0\\0\\0\end{pmatrix} 
\quad \text{or, equivalently}\quad
\begin{pmatrix}
1 &amp; 3 &amp; -1 \\
1 &amp; 2 &amp; -2 \\
-1 &amp; 0 &amp; 4
\end{pmatrix} \begin{pmatrix}x\\y\\z\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix}.\]</span> We apply the following row operations: <span class="math display">\[\begin{aligned}
\left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 3 &amp; -1 \\
1 &amp; 2 &amp; -2 \\
-1 &amp; 0 &amp; 4
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 3 &amp; -1 \\
0 &amp; -1 &amp; -1 \\
0 &amp; 3 &amp; 3
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;&amp;\!\!\!\begin{array}{r@{}l}
r_{2} &amp;{} \mapsto r_{2}-r_{1} \\
r_{3} &amp;{} \mapsto r_{3}+r_{1}
\end{array} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 3 &amp; -1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 3 &amp; 3
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;&amp;r_{2} \mapsto -r_{2} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 3 &amp; -1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;&amp;r_{3} \mapsto r_{3}-3r_{2}\end{aligned}\]</span> Hence our original system of equation is equivalent to <span class="math display">\[\begin{array}{r@{}r@{}r@{}l}
x &amp;{} + 3y &amp;{} - z &amp;{} = 0 \\
&amp; y &amp;{} + z &amp;{} = 0.
\end{array}\]</span> Therefore <span class="math inline">\(y = -z\)</span> and <span class="math inline">\(x = -3y+z = 4z\)</span>. So <span class="math display">\[\ker T = \left\{ \begin{pmatrix}4z\\-z\\z\end{pmatrix} \biggm| z \in \mathbb{R}\right\} =
  \operatorname{Span} \begin{pmatrix}4\\-1\\1\end{pmatrix}.\]</span> Then <span class="math inline">\(\left\{ \begin{pmatrix}4\\-1\\1\end{pmatrix} \right\}\)</span> is a basis for <span class="math inline">\(\ker T\)</span> (as a set containing a single non-zero vector is immediately linearly independent).</p>
<div class="center">
<hr />
</div></li>
<li><p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong> <span class="math display">\[T\begin{pmatrix}
 x \\
 y \\
 z \\
 \end{pmatrix} = \begin{pmatrix}
 x+3y-z \\
 x+2y-2z \\
 -x+4z \\
 \end{pmatrix} =
x\begin{pmatrix}
 1 \\
 1 \\
 -1 \\
 \end{pmatrix} + y\begin{pmatrix}
 3 \\
 2 \\
 0 \\
 \end{pmatrix} + z\begin{pmatrix}
 -1 \\
 -2 \\
 4 \\
 \end{pmatrix},\]</span> so <span class="math display">\[\begin{aligned}
\operatorname{im}T &amp;= \left\{ T\begin{pmatrix}
 x \\
 y \\
 z \\
 \end{pmatrix} \biggm| \begin{pmatrix}
 x \\
 y \\
 z \\
 \end{pmatrix} \in
  \mathbb{R}^{3} \right\} 
= \left\{ x\begin{pmatrix}
 1 \\
 1 \\
 -1 \\
 \end{pmatrix} + y\begin{pmatrix}
 3 \\
 2 \\
 0 \\
 \end{pmatrix} +
  z\begin{pmatrix}
 -1 \\
 -2 \\
 4 \\
 \end{pmatrix} \biggm| x,y,z \in \mathbb{R}\right\} \\
&amp;= \operatorname{Span} \left( \begin{pmatrix}
 1 \\
 1 \\
 -1 \\
 \end{pmatrix},
  \begin{pmatrix}
 3 \\
 2 \\
 0 \\
 \end{pmatrix}, \begin{pmatrix}
 -1 \\
 -2 \\
 4 \\
 \end{pmatrix} \right).\end{aligned}\]</span> We shall determine if this spanning set is linearly independent. We solve <span class="math display">\[\alpha\begin{pmatrix}1\\1\\-1\end{pmatrix} + \beta\begin{pmatrix}3\\2\\0\end{pmatrix} +
\gamma\begin{pmatrix}-1\\-2\\4\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix};\]</span> i.e., <span class="math display">\[\begin{pmatrix}
1 &amp; 3 &amp; -1 \\
1 &amp; 2 &amp; -2 \\
-1 &amp; 0 &amp; 4
\end{pmatrix} \begin{pmatrix}\alpha\\\beta\\\gamma\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix}.\]</span> This is essentially the same equations as we considered before and we showed it was equivalent to <span class="math display">\[\begin{array}{r@{}r@{}r@{}l}
\alpha &amp;{} + 3\beta &amp;{} - \gamma &amp;{} = 0 \\
&amp; \beta &amp;{} + \gamma &amp;{} = 0.
\end{array}\]</span> One solution can then be found by taking <span class="math inline">\(\gamma = 1\)</span>, so <span class="math inline">\(\beta = -1\)</span> and <span class="math inline">\(\alpha = 4\)</span>. Hence <span class="math display">\[4 \begin{pmatrix}1\\1\\-1\end{pmatrix} - \begin{pmatrix}3\\2\\0\end{pmatrix} + \begin{pmatrix}-1\\-2\\4\end{pmatrix} =
\begin{pmatrix}0\\0\\0\end{pmatrix}
\quad\Rightarrow\quad
\begin{pmatrix}-1\\-2\\4\end{pmatrix} = -4 \begin{pmatrix}1\\1\\-1\end{pmatrix} + \begin{pmatrix}3\\2\\0\end{pmatrix}.\]</span> So we deduce <span class="math inline">\(\begin{pmatrix}-1\\-2\\4\end{pmatrix}\)</span> is a linear combination of <span class="math inline">\(\begin{pmatrix}1\\1\\-1\end{pmatrix}\)</span> and <span class="math inline">\(\begin{pmatrix}3\\2\\0\end{pmatrix}\)</span>. Therefore <span class="math display">\[\operatorname{im}T = \operatorname{Span} \left( \begin{pmatrix}1\\1\\-1\end{pmatrix},
  \begin{pmatrix}3\\2\\0\end{pmatrix} \right) .\]</span> It is easy to see the set <span class="math inline">\(\left\{ \begin{pmatrix}1\\1\\-1\end{pmatrix},  \begin{pmatrix}3\\2\\0\end{pmatrix} \right\}\)</span> is linearly independent (for example, neither vector here is a scalar multiple of the other). Hence <span class="math display">\[\left\{ \begin{pmatrix}1\\1\\-1\end{pmatrix}, \begin{pmatrix}3\\2\\0\end{pmatrix} \right\}\]</span> is a basis for <span class="math inline">\(\operatorname{im}T\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p>We have shown <span class="math inline">\(\operatorname{null}{T} = \dim \ker T = 1\)</span> and <span class="math inline">\(\operatorname{rank}{T} = \dim \operatorname{im} T = 2\)</span>. Thus <span class="math inline">\(\operatorname{rank}{T} + \operatorname{null}{T} = 3 = \dim \mathbb{R}^{3}\)</span>, as we expect from the Rank-Nullity Theorem (<a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-rank-nullity">Theorem 4.1.7</a>).</p></li>
</ol>
</div></li>
<li><p><span id="problem-04-02" label="problem-04-02"></span></p>
<div class="questionjupyter">
<p>Define a linear transformation <span class="math inline">\(T \colon \mathbb{R}^{4} \longrightarrow\mathbb{R}^{3}\)</span> by <span class="math display">\[\begin{aligned}
    T(\vec{e}_{1}) &amp;= \vec{y}_{1} = \begin{pmatrix}
      1 \\
      -2 \\ 
      3 \\
    \end{pmatrix},
    &amp;T(\vec{e}_{2}) &amp;= \vec{y}_{2} = \begin{pmatrix}
      -3 \\
      0 \\  
      9 \\
    \end{pmatrix}, \\
    T(\vec{e}_{3}) &amp;= \vec{y}_{3} = \begin{pmatrix}
      -2 \\
      1 \\  
      3 \\
    \end{pmatrix},
    &amp;T(\vec{e}_{4}) &amp;= \vec{y}_{4} = \begin{pmatrix}
      1 \\
      -1 \\ 
      0 \\
    \end{pmatrix},
  \end{aligned}\]</span> where <span class="math inline">\(\{ \vec{e}_{1},\vec{e}_{2},\vec{e}_{3},\vec{e}_{4} \}\)</span> is the standard basis for <span class="math inline">\(\mathbb{R}^{4}\)</span>.</p>
<ol type="1">
<li><p>Find a subset of <span class="math inline">\(\{ \vec{y}_{1}, \vec{y}_{2}, \vec{y}_{3},  \vec{y}_{4} \}\)</span> that is a basis for the image of <span class="math inline">\(T\)</span>.</p></li>
<li><p>Find a basis for the kernel of <span class="math inline">\(T\)</span>.</p></li>
<li><p>Hence determine the rank and nullity of <span class="math inline">\(T\)</span>.</p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong></p>
<p>We need to find how to write vectors in <span class="math inline">\(\{ \vec{y}_{1}, \vec{y}_{2}, \vec{y}_{3}, \vec{y}_{4} \}\)</span> as linear combinations of the others. We solve <span class="math display">\[\alpha \vec{y}_{1} + \beta \vec{y}_{2} + \gamma \vec{y}_{3} +
\delta \vec{y}_{4} = \vec{0};\]</span> that is, <span class="math display">\[\begin{pmatrix}
1 &amp; -3 &amp; -2 &amp; 1 \\
-2 &amp; 0 &amp; 1 &amp; -1 \\
3 &amp; 9 &amp; 3 &amp; 0
\end{pmatrix}
\begin{pmatrix}\alpha\\\beta\\\gamma\\\delta\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix}.\]</span> We apply row operations to the following augmented matrix: <span class="math display">\[\begin{aligned}
\left( \begin{matrix}
1 &amp; -3 &amp; -2 &amp; 1 \\
-2 &amp; 0 &amp; 1 &amp; -1 \\
3 &amp; 9 &amp; 3 &amp; 0
\end{matrix}
\;\middle|\;
\begin{matrix}
0 \\ 0 \\ 0
\end{matrix} \right)
&amp;\longrightarrow
\left( \begin{matrix}
1 &amp; -3 &amp; -2 &amp; 1 \\
0 &amp; -6 &amp; -3 &amp; 1 \\
0 &amp; 18 &amp; 9 &amp; -3
\end{matrix}
\;\middle|\;
\begin{matrix}
0 \\ 0 \\ 0
\end{matrix} \right)
&amp;&amp;\begin{array}{l}
r_{2} \mapsto r_{2} + 2r_{1} \\
r_{3} \mapsto r_{3} - 3r_{1}
\end{array} \\
&amp;\longrightarrow
\left( \begin{matrix}
1 &amp; -3 &amp; -2 &amp; 1 \\
0 &amp; -6 &amp; -3 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0
\end{matrix}
\;\middle|\;
\begin{matrix}
0 \\ 0 \\ 0
\end{matrix} \right)
&amp;&amp;\begin{array}{l}
r_{3} \mapsto r_{3} + 3r_{2}
\end{array}\end{aligned}\]</span> So the original system of equations is equivalent to <span class="math display">\[\begin{aligned}
\alpha - 3\beta - 2\gamma + \delta &amp;= 0 \\
-6\beta - 3\gamma + \delta &amp;= 0.\end{aligned}\]</span> It is most convenient here to view <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> as free variables and read off solutions for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\delta\)</span>. In particular, if we take <span class="math inline">\(\beta = 1\)</span> and <span class="math inline">\(\gamma = 0\)</span>, then <span class="math display">\[\delta = 6\beta + 3\gamma = 6 \qquad \text{and} \qquad
\alpha = 3\beta + 2\gamma - \delta = -3.\]</span> Hence <span class="math display">\[-3\vec{y}_{1} + \vec{y}_{2} + 6\vec{y}_{4} = \vec{0}\quad\text{and so}\quad
\vec{y}_{2} = 3\vec{y}_{1} - 6\vec{y}_{4}.\]</span> If we take <span class="math inline">\(\beta = 0\)</span> and <span class="math inline">\(\gamma = 1\)</span>, then <span class="math display">\[\delta = 6\beta + 3\gamma = 3 \qquad \text{and} \qquad
\alpha = 3\beta + 2\gamma - \delta = -1.\]</span> Hence <span class="math display">\[-\vec{y}_{1} + \vec{y}_{3} + 3\vec{y}_{4} = \vec{0}
\quad\text{and so}\quad
\vec{y}_{3} = \vec{y}_{1} - 3\vec{y}_{4}.\]</span> These two observations tell us that <span class="math inline">\(\vec{y}_{2},\vec{y}_{3} \in \operatorname{Span}(\vec{y}_{1},\vec{y}_{4})\)</span>, so <span class="math display">\[\operatorname{im}T = \operatorname{Span}(\vec{y}_{1},\vec{y}_{2},\vec{y}_{3},\vec{y}_{4}) =
\operatorname{Span}(\vec{y}_{1},\vec{y}_{4}).\]</span></p>
<p>Finally, if <span class="math inline">\(\alpha \vec{y}_{1} + \beta \vec{y}_{4} = \vec{0}\)</span>, then <span class="math display">\[\begin{aligned}
\alpha + \beta &amp;= 0 \\
-2\alpha - \beta &amp;= 0 \\
3\alpha \quad\;\;\; &amp;= 0,\end{aligned}\]</span> so <span class="math inline">\(\alpha = \beta = 0\)</span>. Hence <span class="math inline">\(\{ \vec{y}_{1},\vec{y}_{4} \}\)</span> is linearly independent and so this is a basis for <span class="math inline">\(\operatorname{im}T\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong></p>
<p>We calculate <span class="math display">\[\begin{aligned}
T\begin{pmatrix}x\\y\\z\\t\end{pmatrix} &amp;= T(x\vec{e}_{1} + y\vec{e}_{2} + z\vec{e}_{3} + t\vec{e}_{4}) \\
&amp;= x\begin{pmatrix}1\\-2\\3\end{pmatrix} + y\begin{pmatrix}-3\\0\\9\end{pmatrix} +
z\begin{pmatrix}-2\\1\\3\end{pmatrix} + t\begin{pmatrix}1\\-1\\0\end{pmatrix} \\
&amp;= \begin{pmatrix}x-3y-2z+t\\-2x+z-t\\3x+9y+3z\end{pmatrix}.\end{aligned}\]</span> We now solve <span class="math inline">\(T\begin{pmatrix}x\\y\\z\\t\end{pmatrix} = \vec{0}\)</span>; that is, <span class="math display">\[\begin{pmatrix}
1 &amp; -3 &amp; -2 &amp; 1 \\
-2 &amp; 0 &amp; 1 &amp; -1 \\
3 &amp; 9 &amp; 3 &amp; 0
\end{pmatrix} \begin{pmatrix}
 x \\
 y \\
 z \\
 t \\
 \end{pmatrix}
= \begin{pmatrix}
 0 \\
 0 \\
 0 \\
 \end{pmatrix}.\]</span> This is (essentially) the same equation as in part (a), so the same row operations will produce the equivalent equations: <span class="math display">\[\begin{aligned}
x - 3y - 2z + t &amp;= 0 \\
-6y - 3z + t &amp;= 0.\end{aligned}\]</span> Hence <span class="math display">\[t = 6y + 3z \qquad \text{and} \qquad x = 3y + 2z - t = -3y - z.\]</span> Therefore <span class="math display">\[\begin{aligned}
\ker T &amp;= \left\{ \begin{pmatrix}-3y-z\\y\\z\\6y+3z\end{pmatrix} \;\middle|\; y,z \in \mathbb{R}
\right\} 
= \left\{ y\begin{pmatrix}-3\\1\\0\\6\end{pmatrix} + z\begin{pmatrix}-1\\0\\1\\3\end{pmatrix}
  \;\middle|\; y,z \in \mathbb{R}\right\} 
= \operatorname{Span} \left( \begin{pmatrix}-3\\1\\0\\6\end{pmatrix},
  \begin{pmatrix}-1\\0\\1\\3\end{pmatrix} \right).\end{aligned}\]</span> If we solve <span class="math display">\[\alpha \begin{pmatrix}-3\\1\\0\\6\end{pmatrix} + \beta \begin{pmatrix}-1\\0\\1\\3\end{pmatrix} = \begin{pmatrix}0\\0\\0\\0\end{pmatrix},\]</span> we immediately see that <span class="math inline">\(\alpha = \beta = 0\)</span>. Hence the set of the above two vectors is linearly independent. We conclude that <span class="math display">\[\left\{ \begin{pmatrix}-3\\1\\0\\6\end{pmatrix}, \begin{pmatrix}-1\\0\\1\\3\end{pmatrix} \right\}\]</span> is a basis for <span class="math inline">\(\ker T\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p><span class="math inline">\(\operatorname{rank}T = \dim \operatorname{im}T = 2\)</span> (from (a)) and <span class="math inline">\(\operatorname{null}T = \dim \ker T = 2\)</span> (from (b)).</p></li>
</ol>
</div></li>
<li><p><span id="problem-04-03" label="problem-04-03"></span></p>
<div class="question">
<p>Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite-dimensional vector spaces over a field <span class="math inline">\(F\)</span>. An <em>isomorphism</em> between <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> is a linear transformation <span class="math inline">\(T \colon V \longrightarrow W\)</span> which is invertible (that is, there is an inverse, a linear transformation <span class="math inline">\(S \colon W \longrightarrow V\)</span> such that <span class="math inline">\(ST = I\)</span> and <span class="math inline">\(TS = I\)</span> are the identity maps). We say that <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are <em>isomorphic</em>, written <span class="math inline">\(V \cong W\)</span>, if there exists an isomorphism <span class="math inline">\(V \longrightarrow W\)</span>.</p>
<ol type="1">
<li><p>Show that a linear transformation <span class="math inline">\(T \colon V \longrightarrow W\)</span> is injective (that is, <span class="math inline">\(T(u) = T(v)\)</span> implies <span class="math inline">\(u = v\)</span>) if and only if <span class="math inline">\(\ker T = \{\vec{0}\}\)</span>.</p></li>
<li><p>Show that a linear transformation <span class="math inline">\(T \colon V \longrightarrow W\)</span> is an isomorphism if and only if it is bijective (that is, both injective and surjective (which means <span class="math inline">\(\operatorname{im} T = W\)</span>)).</p></li>
<li><p>If <span class="math inline">\(\mathscr{B} = \{ v_{1}, v_{2}, \ldots, v_{n} \}\)</span> is a basis for <span class="math inline">\(V\)</span> and <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2},\ldots,w_{n} \}\)</span> is a basis for <span class="math inline">\(W\)</span>, show that the unique linear transformation <span class="math inline">\(T \colon V \longrightarrow W\)</span> given by <span class="math inline">\(T(v_{i}) = w_{i}\)</span>, for <span class="math inline">\(i = 1\)</span>, <span class="math inline">\(2\)</span>, …, <span class="math inline">\(n\)</span>, is an isomorphism.</p>
<p>Deduce that <span class="math inline">\(V \cong W\)</span> if and only if <span class="math inline">\(\dim V = \dim W\)</span>.</p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p>(<span class="math inline">\(\Rightarrow\)</span>) Suppose <span class="math inline">\(T\)</span> is injective. If <span class="math inline">\(v \in \ker T\)</span>, then <span class="math inline">\(T(v) = \vec{0} = T(\vec{0})\)</span> and so <span class="math inline">\(v = \vec{0}\)</span>, because <span class="math inline">\(T\)</span> is injective. This shows <span class="math inline">\(\ker T = \{\vec{0}\}\)</span>.</p>
<p>(<span class="math inline">\(\Leftarrow\)</span>) Suppose <span class="math inline">\(\ker T = \{\vec{0}\}\)</span>. If <span class="math inline">\(T(u) = T(v)\)</span>, then <span class="math inline">\(T(u-v) = T(u) - T(v) =  \vec{0}\)</span>, and so <span class="math inline">\(u-v \in \ker T = \{\vec{0}\}\)</span>. Hence <span class="math inline">\(u-v = \vec{0}\)</span>, and so <span class="math inline">\(u = v\)</span>. This shows that <span class="math inline">\(T\)</span> is injective.</p>
<div class="center">
<hr />
</div></li>
<li><p>Suppose <span class="math inline">\(T \colon V \longrightarrow W\)</span> is an isomorphism. This means there is a linear transformation <span class="math inline">\(S \colon W \longrightarrow V\)</span> such that <span class="math inline">\(ST = \operatorname{id}_V  \colon V \longrightarrow V\)</span> and <span class="math inline">\(TS = \operatorname{id}_W \colon W \longrightarrow W\)</span> are the identity maps.</p>
<p>(<span class="math inline">\(\Rightarrow\)</span>) Let <span class="math inline">\(v \in \ker T\)</span>. Then <span class="math inline">\(v = ST(v) = S(\vec{0}) = \vec{0}\)</span>, so <span class="math inline">\(\ker T = \{\vec{0}\}\)</span> and we deduce (from (a)) that <span class="math inline">\(T\)</span> is injective. If <span class="math inline">\(w \in W\)</span>, then put <span class="math inline">\(u = S(w) \in V\)</span>. Then <span class="math inline">\(T(u) = TS(w) = w\)</span>, so <span class="math inline">\(T\)</span> is surjective. We conclude that if <span class="math inline">\(T\)</span> is an isomorphism, then it is bijective.</p>
<p>(<span class="math inline">\(\Leftarrow\)</span>) Suppose <span class="math inline">\(T \colon V \longrightarrow W\)</span> is a bijective linear transformation. Then, given <span class="math inline">\(w \in W\)</span>, there exists a <em>unique</em> <span class="math inline">\(v \in V\)</span> satisfying <span class="math inline">\(T(v) = w\)</span>. Define a map <span class="math inline">\(S \colon W \longrightarrow V\)</span> by <span class="math inline">\(S(w) = v\)</span>. We shall verify that <span class="math inline">\(S\)</span> is a linear map which is an inverse for <span class="math inline">\(T\)</span>.</p>
<p>First note that, by definition, if <span class="math inline">\(w \in W\)</span> and <span class="math inline">\(v\)</span> is the unique vector in <span class="math inline">\(V\)</span> satisfying <span class="math inline">\(T(v) = w\)</span>, then <span class="math display">\[TS(w) = T(v) = w.\]</span> We deduce that <span class="math inline">\(TS(w) = w\)</span> for all <span class="math inline">\(w \in W\)</span> and so, <span class="math inline">\(TS = \operatorname{id}_W\)</span>.</p>
<p>If <span class="math inline">\(w_{1},w_{2} \in W\)</span>, let <span class="math inline">\(v_{1},v_{2} \in V\)</span> be the unique vectors satisfying <span class="math inline">\(T(v_{1}) = w_{1}\)</span> and <span class="math inline">\(T(v_{2}) = w_{2}\)</span>. Then <span class="math display">\[T(v_{1}+v_{2}) = T(v_{1}) + T(v_{2}) = w_{1} + w_{2}\]</span> and therefore <span class="math inline">\(v_{1} + v_{2}\)</span> must be the unique vector satisfying this equation. Hence <span class="math display">\[S(w_{1}+w_{2}) = v_{1} + v_{2} = S(w_{1}) + S(w_{2}).\]</span> Equally if <span class="math inline">\(\alpha\)</span> is a scalar, then <span class="math display">\[T(\alpha v_{1}) = \alpha T(v_{1}) = \alpha w_{1},\]</span> so <span class="math inline">\(\alpha v_{1}\)</span> is the unique vector mapping to <span class="math inline">\(\alpha w_{1}\)</span> under <span class="math inline">\(T\)</span>. Hence <span class="math display">\[S(\alpha w_{1}) = \alpha v_{1} = \alpha S(w_{1}).\]</span> This shows that <span class="math inline">\(S\)</span> is a linear transformation.</p>
<p>Finally if <span class="math inline">\(v \in V\)</span>, put <span class="math inline">\(w = T(v)\)</span>. Then, by definition, we have <span class="math inline">\(S(w) = v\)</span>, so <span class="math display">\[ST(v) = S(w) = v.\]</span> We deduce that <span class="math inline">\(ST = \operatorname{id}_V\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p>Consider the unique linear transformation <span class="math inline">\(T \colon V \longrightarrow W\)</span> given by <span class="math display">\[T(v_{i}) = w_{i} \qquad \text{for $i = 1$, $2$, \ldots, $n$}.\]</span> By <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a>, <span class="math display">\[\operatorname{im}T = \operatorname{Span}(w_{1},w_{2},\ldots,w_{n}) = W\]</span> and <span class="math display">\[\ker T = \{\vec{0}\},\]</span> since <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2},\ldots,w_{n} \}\)</span> is linearly independent. Hence <span class="math inline">\(T\)</span> is bijective and therefore an isomorphism (by (a) and (b)).</p>
<p>(<span class="math inline">\(\Leftarrow\)</span>) Suppose <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are vector spaces with <span class="math inline">\(\dim V = \dim W\)</span>. Then there exist bases <span class="math inline">\(\mathscr{B}\)</span> and <span class="math inline">\(\mathscr{C}\)</span> for <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, respectively, of the same size. We may therefore define an isomorphism <span class="math inline">\(T \colon V \longrightarrow W\)</span> as above and deduce <span class="math inline">\(V \cong W\)</span>.</p>
<p>(<span class="math inline">\(\Rightarrow\)</span>) Suppose <span class="math inline">\(V \cong W\)</span>. Then there exists an isomorphism <span class="math inline">\(T \colon V \longrightarrow W\)</span>. Let <span class="math inline">\(\mathscr{B}= \{ v_{1},v_{2},\ldots,v_{n} \}\)</span> be a basis for <span class="math inline">\(V\)</span>. Put <span class="math display">\[w_{i} = T(v_{i}) \qquad \text{for $i = 1$, $2$, \ldots, $n$}.\]</span> Now as <span class="math inline">\(\ker T = \{\vec{0}\}\)</span> and <span class="math inline">\(\operatorname{im}T = W\)</span> (as <span class="math inline">\(T\)</span> is bijective), we may apply <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-surj-inj">Proposition 4.1.8</a> to see that <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2},\ldots,w_{n} \}\)</span> is a linearly independent spanning set; that is, a basis for <span class="math inline">\(W\)</span>. Hence <span class="math display">\[\dim W = n = \dim V,\]</span> as required.</p></li>
</ol>
</div></li>
<li><p><span id="problem-04-04" label="problem-04-04"></span></p>
<div class="question">
<p>Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be vector spaces over the field <span class="math inline">\(F\)</span>. If <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> are linear transformations <span class="math inline">\(V \longrightarrow W\)</span>, show that</p>
<ol type="1">
<li><p>the sum <span class="math inline">\(S+T\)</span>, defined by <span class="math inline">\((S+T)(v) = S(v) + T(v)\)</span>, is a linear transformation, and</p></li>
<li><p>the scalar multiple <span class="math inline">\(\alpha T\)</span>, defined by <span class="math inline">\((\alpha T)(v) = \alpha \cdot T(v)\)</span>, is a linear transformation (for any scalar <span class="math inline">\(\alpha \in F\)</span>).</p></li>
</ol>
<p>Write <span class="math inline">\(\mathcal{L}(V,W)\)</span> for the set of all linear transformations <span class="math inline">\(V \longrightarrow W\)</span>. Show that <span class="math inline">\(\mathcal{L}(V,W)\)</span> is itself a vector space over <span class="math inline">\(F\)</span> with the above addition and scalar multiplication.</p>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p>Let <span class="math inline">\(u,v \in V\)</span> and <span class="math inline">\(\alpha \in F\)</span>. Then <span class="math display">\[\begin{gathered}
(S+T)(u+v) = S(u+v) + T(u+v) = S(u) + S(v) + T(u) + T(v) \\
{} = S(u) + T(u) + S(v) + T(v) = (S+T)(u) + (S+T)(v)\end{gathered}\]</span> and <span class="math display">\[(S+T)(\alpha v) = S(\alpha v) + T(\alpha v) = \alpha \, S(v) + \alpha
\, T(v) = \alpha (S(v)+T(v)) = \alpha \, (S+T)(v).\]</span> Hence <span class="math inline">\(S+T\)</span> is a linear transformation.</p>
<div class="center">
<hr />
</div></li>
<li><p>Let <span class="math inline">\(u,v \in V\)</span> and <span class="math inline">\(\beta \in F\)</span>. Then <span class="math display">\[(\alpha T)(u+v) = \alpha \, T(u+v) = \alpha (T(u) + T(v)) = \alpha
T(u) + \alpha  T(v)\]</span> and <span class="math display">\[(\alpha T)(\beta v) = \alpha \, T(\beta v) = \alpha \beta \, T(v) =
\beta \, \alpha T(v) = \beta \, (\alpha T)(v).\]</span> Hence <span class="math inline">\(\alpha T\)</span> is a linear transformation.</p>
<p>We have shown that if <span class="math inline">\(S,T \in \mathcal{L}(V,W)\)</span> and <span class="math inline">\(\alpha \in F\)</span>, then <span class="math inline">\(S+T \in \mathcal{L}(V,W)\)</span> and <span class="math inline">\(\alpha T \in \mathcal{L}(V,W)\)</span>. It remains to show that, with this addition and scalar multiplication, the conditions from <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#def-vspace">Definition 2.1.3</a>.</p>
<p>Let <span class="math inline">\(R,S,T \colon V \longrightarrow W\)</span> and <span class="math inline">\(\alpha,\beta \in F\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\((S+T)(v) = S(v) + T(v) = T(v) + S(v) = (T+S)(v)\)</span> for all <span class="math inline">\(v \in V\)</span>, so we deduce <span class="math inline">\(S+T = T+S\)</span>.</p></li>
<li><p><span class="math display">\[\begin{gathered}
\left( (R+S) + T \right) (v) = (R+S)(v) + T(v) = \left( R(v) + S(v)
\right) + T(v) \\
{} = R(v) + \left( S(v) + T(v) \right) = R(v) + (S+T)(v) =
\left( R + (S+T) \right) (v)\end{gathered}\]</span> for all <span class="math inline">\(v \in V\)</span>, so we deduce <span class="math inline">\((R+S)+T = R+(S+T)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(Z \colon V \longrightarrow W\)</span> be given by <span class="math inline">\(Z(v) = \vec{0}\)</span> for all <span class="math inline">\(v \in V\)</span>. Note that <span class="math inline">\(Z\)</span> is a linear transformation (as <span class="math inline">\(Z(u+v) = \vec{0} = \vec{0} + \vec{0} = Z(u) + Z(v)\)</span> and <span class="math inline">\(Z(\beta v) = \vec{0} = \beta \, \vec{0} = \beta \, Z(v)\)</span> for all <span class="math inline">\(u,v \in V\)</span> and <span class="math inline">\(\beta \in F\)</span>) and <span class="math display">\[(T+Z)(v) = T(v) + Z(v) = T(v) + \vec{0} = T(v)\]</span> for all <span class="math inline">\(v \in V\)</span>, so <span class="math inline">\(T+Z = T\)</span> for all <span class="math inline">\(T \in \mathcal{L}(V,W)\)</span>. Hence <span class="math inline">\(Z\)</span> plays the role of the zero vector in <span class="math inline">\(\mathcal{L}(V,W)\)</span>, and we shall therefore now denote it <span class="math inline">\(0\)</span>. So we have shown <span class="math inline">\(T+0 = T\)</span> where <span class="math inline">\(0\)</span> here is the zero map given by <span class="math inline">\(0 \colon v \mapsto \vec{0}\)</span> for all <span class="math inline">\(v\)</span>.</p></li>
<li><p>Taking <span class="math inline">\(\alpha = -1\)</span> above shows that the mapping <span class="math inline">\(-T\)</span> given by <span class="math inline">\(v \mapsto -T(v) = (-1)T(v)\)</span> is a linear transformation in <span class="math inline">\(\mathcal{L}(V,W)\)</span>. We then have <span class="math display">\[(T + (-T))(v) = T(v) - T(v) = \vec{0}\]</span> for all <span class="math inline">\(v \in V\)</span>, so <span class="math inline">\(T + (-T) = 0\)</span>.</p></li>
<li><p><span class="math display">\[\alpha (S+T)(v) = \alpha (S(v) + T(v)) = \alpha \, S (v) + \alpha \,
T(v) = (\alpha S + \alpha T)(v)\]</span> for all <span class="math inline">\(v \in V\)</span>, so <span class="math inline">\(\alpha(S+T) = \alpha S + \alpha T\)</span>.</p></li>
<li><p><span class="math display">\[(\alpha + \beta)T(v) = \alpha T(v) + \beta T(v) = (\alpha T + \beta T)(v)\]</span> for all <span class="math inline">\(v \in V\)</span>, so <span class="math inline">\((\alpha + \beta)T = \alpha T + \beta T\)</span>.</p></li>
<li><p><span class="math display">\[(\alpha \beta)T(v) = \alpha \left( \beta T(v) \right) = \left(
  \alpha(\beta T) \right)(v)\]</span> for all <span class="math inline">\(v \in V\)</span>, so <span class="math inline">\((\alpha \beta)T = \alpha(\beta T)\)</span>.</p></li>
<li><p><span class="math inline">\(1 T(v) = T(v)\)</span> for all <span class="math inline">\(v \in V\)</span>, so <span class="math inline">\(1T = T\)</span>.</p></li>
</ol>
<p>Hence <span class="math inline">\(\mathcal{L}(V,W)\)</span> is indeed a vector space over <span class="math inline">\(F\)</span>.</p></li>
</ol>
</div></li>
<li><p><span id="problem-04-05" label="problem-04-05"></span></p>
<div class="question">
<p>Let <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be vector spaces over the field <span class="math inline">\(F\)</span>. If <span class="math inline">\(T \colon U \longrightarrow V\)</span> and <span class="math inline">\(S \colon V \longrightarrow W\)</span> are linear transformations, show that the composition <span class="math display">\[\begin{aligned}
ST \colon U &amp;\longrightarrow W \\
u &amp;\mapsto S(Tu)\end{aligned}\]</span> is also a linear transformation.</p>
<p>If we fix bases <span class="math inline">\(\mathscr{A}\)</span>, <span class="math inline">\(\mathscr{B}\)</span> and <span class="math inline">\(\mathscr{C}\)</span> for <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, respectively, show that <span class="math display">\[\operatorname{Mat}_{\mathscr{A},\mathscr{C}}(ST) = \operatorname{Mat}_{\mathscr{B},\mathscr{C}}(S) \cdot \operatorname{Mat}_{\mathscr{A},\mathscr{B}}(T).\]</span></p>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<p>Let <span class="math inline">\(u,v \in U\)</span>. Then <span class="math display">\[ST(u+v) = S(T(u+v)) = S(T(u) + T(v)) = S(T(u)) + S(T(v)) = ST(u) + ST(v)\]</span> and if also <span class="math inline">\(\alpha \in F\)</span> <span class="math display">\[ST(\alpha v) = S(T(\alpha v)) = S(\alpha\, T(v)) = \alpha \, S(T(v)) =
\alpha \, ST(v).\]</span> Hence the composition <span class="math inline">\(ST\)</span> of two linear transformations is linear.</p>
<p>Let <span class="math inline">\(\mathscr{A}= \{ u_{1},u_{2},\ldots,u_{\ell} \}\)</span>,  <span class="math inline">\(\mathscr{B}= \{ v_{1},v_{2},\ldots,v_{m} \}\)</span> and <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2},\ldots,w_{n} \}\)</span> be bases for the vector spaces <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, respectively. Let <span class="math inline">\(A = [\alpha_{ij}] = \operatorname{Mat}_{\mathscr{B},\mathscr{C}}(S)\)</span> and <span class="math inline">\(B = [\beta_{ij}] = \operatorname{Mat}_{\mathscr{A},\mathscr{B}}(T)\)</span>. This means <span class="math display">\[T(u_{j}) = \sum_{i=1}^{m} \beta_{ij} v_{i} \qquad \text{for $j=1$,
  $2$, \ldots, $\ell$}\]</span> and <span class="math display">\[S(v_{j}) = \sum_{i=1}^{n} \alpha_{ij} w_{i} \qquad \text{for $j=1$,
  $2$, \ldots, $m$}.\]</span> Thus <span class="math display">\[\begin{aligned}
  ST(u_{j}) &amp;= S \biggl( \sum_{k=1}^{m} \beta_{kj} v_{k} \biggr) 
  = \sum_{k=1}^{m} \beta_{kj} S(v_{k}) 
  = \sum_{k=1}^{n} \beta_{kj} \sum_{i=1}^{n} \alpha_{ik}w_{i} 
  = \sum_{i=1}^{n} \biggl( \sum_{k=1}^{m} \alpha_{ik}\beta_{kj}
  \biggr) w_{i}.\end{aligned}\]</span> Hence the <span class="math inline">\((i,j)\)</span>th entry of <span class="math inline">\(\operatorname{Mat}_{\mathscr{A},\mathscr{C}}(ST)\)</span> is <span class="math display">\[\sum_{k=1}^{m} \alpha_{ik} \beta_{kj}.\]</span> Thus <span class="math display">\[\operatorname{Mat}_{\mathscr{A},\mathscr{C}}(ST) = AB = \operatorname{Mat}_{\mathscr{B},\mathscr{C}}(S) \cdot
\operatorname{Mat}_{\mathscr{A},\mathscr{B}}(T).\square\]</span></p>
</div></li>
<li><p><span id="problem-04-06" label="problem-04-06"></span></p>
<div class="question">
<ol type="1">
<li><p>Let <span class="math inline">\(F\)</span> be a field and <span class="math inline">\(A\)</span> be an <span class="math inline">\(m \times n\)</span> matrix. Show that <span class="math inline">\(A\)</span> defines a linear transformation <span class="math inline">\(F^{n} \longrightarrow F^{m}\)</span> by <span class="math display">\[\vec{v} \mapsto A\vec{v} \qquad \text{for $\vec{v} \in F^{n}$.}\]</span></p>
<p>Show that the matrix of this linear transformation with respect to the standard bases of <span class="math inline">\(F^{n}\)</span> and <span class="math inline">\(F^{m}\)</span> is <span class="math inline">\(A\)</span>.</p></li>
<li><p>Now consider any linear transformation <span class="math inline">\(T \colon F^{n} \longrightarrow F^{m}\)</span>. Show that <span class="math display">\[T(\vec{v}) = A\vec{v} \qquad \text{for all $\vec{v} \in F^{n}$,}\]</span> where <span class="math inline">\(A\)</span> is the matrix of <span class="math inline">\(T\)</span> with respect to the standard bases for <span class="math inline">\(F^{n}\)</span> and <span class="math inline">\(F^{m}\)</span>. [Thus, every linear transformation from <span class="math inline">\(F^{n}\)</span> to <span class="math inline">\(F^{m}\)</span> is given by matrix multiplication.]</p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p> Let <span class="math inline">\(\vec{v} = \begin{pmatrix} x_{1} \\ \vdots \\  x_{n} \end{pmatrix}\)</span> and <span class="math inline">\(\vec{w} = \begin{pmatrix} y_{1} \\ \vdots  \\ y_{n} \end{pmatrix}\)</span>. If <span class="math inline">\(A = [a_{ij}]\)</span>, the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(A\vec{v}\)</span> is <span class="math inline">\(\sum_{j=1}^{n} a_{ij}x_{j}\)</span>. Similarly the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(A(\vec{v}+\vec{w})\)</span> is <span class="math display">\[\sum_{j=1}^{n} a_{ij} (x_{j} + y_{j}) = \sum_{j=1}^{n} a_{ij}x_{j} +
\sum_{j=1}^{n} a_{ij}y_{j}\]</span> and hence <span class="math inline">\(A(\vec{v} + \vec{w}) = A\vec{v} + A\vec{w}\)</span>. Equally if <span class="math inline">\(\alpha \in F\)</span>, the <span class="math inline">\(i\)</span>th entry of <span class="math inline">\(A(\alpha\vec{v})\)</span> is <span class="math display">\[\sum_{j=1}^{n} a_{ij}(\alpha x_{j}) = \alpha \sum_{j=1}^{n} a_{ij}x_{j},\]</span> so <span class="math inline">\(A(\alpha \vec{v}) = \alpha \, A\vec{v}\)</span>. Hence <span class="math inline">\(\vec{v} \mapsto A\vec{v}\)</span> is a linear transformation.</p>
<p>[Alternatively, this could all be done extremely quickly by simply quoting standard facts about matrix multiplication.]</p>
<p>It remains to show that the matrix <span class="math inline">\(\operatorname{Mat}_{}(T)\)</span> of the linear transformation <span class="math inline">\(T: F ^ n \longrightarrow F ^ m\)</span> defined by <span class="math inline">\(T(\vec{v}) = A \vec{v}\)</span> is <span class="math inline">\(A\)</span>. If <span class="math inline">\(\vec{e}_i\)</span> is the <span class="math inline">\(i\)</span>-th standard basis vector for <span class="math inline">\(F ^ n\)</span>, then the <span class="math inline">\(i\)</span>-th column of <span class="math inline">\(\operatorname{Mat}_{}(T)\)</span> consists of the coefficients of the standard basis vectors for <span class="math inline">\(F ^ m\)</span> in the unique linear combination that equals <span class="math inline">\(T(\vec{e}_i)\)</span>. But <span class="math inline">\(T(\vec{e}_i)\)</span> is <span class="math display">\[Ae_i = \begin{pmatrix}
           \alpha_{1i} \\
           \alpha_{2i} \\
           \vdots \\
           \alpha_{mi} \\
         \end{pmatrix}\]</span> which is precisely the <span class="math inline">\(i\)</span>-th column of <span class="math inline">\(A\)</span>. Hence every column of <span class="math inline">\(\operatorname{Mat}_{}(T)\)</span> equals the corresponding column of <span class="math inline">\(A\)</span>, and so <span class="math inline">\(\operatorname{Mat}_{}(T) = A\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p> Let <span class="math inline">\(T \colon F^{n} \longrightarrow F^{m}\)</span> and let <span class="math inline">\(A = \operatorname{Mat}_{\mathscr{E},\mathscr{F}}(T)\)</span> where <span class="math inline">\(\mathscr{E} = \{ \vec{e}_{1},\vec{e}_{2},\ldots,\vec{e}_{n} \}\)</span> and <span class="math inline">\(\mathscr{F} = \{ \vec{e}&#39;_{1},\vec{e}&#39;_{2},\ldots,\vec{e}&#39;_{m} \}\)</span> are the standard bases for <span class="math inline">\(F^{n}\)</span> and <span class="math inline">\(F^{m}\)</span>, respectively. Write <span class="math inline">\(A = [\alpha_{ij}]\)</span>, so <span class="math display">\[T(\vec{e}_{j}) = \sum_{i=1}^{m} \alpha_{ij} \vec{e}&#39;_{i}\]</span> by definition. Now if <span class="math inline">\(\vec{v} = \begin{pmatrix} x_{1} \\ \vdots  \\ x_{n} \end{pmatrix} \in F^{n}\)</span>, then we see <span class="math display">\[\begin{aligned}
T(\vec{v}) &amp;= T( x_{1}\vec{e}_{1} + x_{2}\vec{e}_{2} + \cdots + x_{n}\vec{e}_{n} )
= \sum_{j=1}^{n} x_{j} \, T(\vec{e}_{j}) 
= \sum_{j=1}^{n} x_{j} \sum_{i=1}^{m} \alpha_{ij} \vec{e}&#39;_{i}
= \sum_{i=1}^{m} \biggl( \sum_{j=1}^{n} \alpha_{ij} x_{j} \biggr)
\vec{e}&#39;_{i} \\
&amp;= \begin{pmatrix}
\alpha_{11}x_{1} + \alpha_{12}x_{2} + \cdots + \alpha_{1n}x_{n} \\
\alpha_{21}x_{1} + \alpha_{22}x_{2} + \cdots + \alpha_{2n}x_{n} \\
\vdots \\
\alpha_{m1}x_{1} + \alpha_{m2}x_{2} + \cdots + \alpha_{mn}x_{n} \\
\end{pmatrix} 
= \begin{pmatrix}
\alpha_{11} &amp; \alpha_{12} &amp; \cdots &amp; \alpha_{1n} \\
\alpha_{12} &amp; \alpha_{22} &amp; \cdots &amp; \alpha_{2n} \\
\vdots      &amp; \vdots      &amp; \ddots &amp; \vdots \\
\alpha_{m1} &amp; \alpha_{m2} &amp; \cdots &amp; \alpha_{mn}
\end{pmatrix}
\begin{pmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{pmatrix} 
= A\vec{v}.\square\end{aligned}\]</span></p></li>
</ol>
</div></li>
<li><p><span id="problem-04-07" label="problem-04-07"></span></p>
<div class="question">
<p>Let <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> be finite-dimensional vector spaces over the field <span class="math inline">\(F\)</span> with bases <span class="math inline">\(\mathscr{B} = \{ v_{1},v_{2},\dots,v_{n} \}\)</span> and <span class="math inline">\(\mathscr{C}  = \{ w_{1},w_{2},\dots,w_{m} \}\)</span>, respectively. Define linear transformations <span class="math inline">\(T_{k\ell} \colon V \longrightarrow W\)</span> by the following formula: <span class="math display">\[T_{k\ell}(v_{j}) = 
      \begin{cases}
        w_{k}   &amp; \text{if }j = l, \\
        \vec{0} &amp; \text{if }j \neq l.
      \end{cases}\]</span></p>
<ol type="1">
<li><p>Show that every linear transformation <span class="math inline">\(T \colon V \longrightarrow W\)</span> can be expressed as <span class="math display">\[T = \sum_{\substack{1 \leqslant k \leqslant m\\1 \leqslant\ell \leqslant
          n}} \alpha_{k\ell} T_{k\ell}\]</span> for some scalars <span class="math inline">\(\alpha_{k\ell} \in F\)</span>. [See Problem <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#problem-04-04">4</a> for the definition of sums and scalar multiples of linear transformation.]</p></li>
<li><p>Let <span class="math inline">\(\mathscr{A} = \{T_{k\ell} : 1 \leqslant k \leqslant m, \; 1 \leqslant\ell \leqslant  n\}\)</span>. Prove that <span class="math inline">\(\mathscr{A}\)</span> is a linearly independent subset of <span class="math inline">\(\mathcal{L}(V,W)\)</span> and deduce that <span class="math display">\[\dim \mathcal{L}(V,W) = \dim V \cdot \dim W.\]</span></p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p> Let <span class="math inline">\(T \colon V \longrightarrow W\)</span> be any linear transformation and let <span class="math inline">\(B\)</span> be the matrix of <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathscr{B}\)</span> and <span class="math inline">\(\mathscr{C}\)</span>, say <span class="math inline">\(B = [\beta_{ij}]\)</span>. This means, of course, <span class="math display">\[T(v_{j}) = \sum_{i=1}^{m} \beta_{ij} w_{i}\]</span> for each <span class="math inline">\(j\)</span>.</p>
<p>Consider the transformation <span class="math inline">\(S = \sum \alpha_{k\ell} T_{k\ell}\)</span>, where the summation is taken over the range <span class="math inline">\(1 \leqslant k \leqslant n\)</span>,  <span class="math inline">\(1 \leqslant \ell \leqslant m\)</span>. Since <span class="math inline">\(T_{k\ell}(v_{j}) = \vec{0}\)</span> when <span class="math inline">\(k \neq j\)</span> while <span class="math inline">\(T_{j\ell}(v_{j}) = w_{\ell}\)</span>, we conclude <span class="math display">\[S(v_{j}) = \sum_{\substack{1 \leqslant k \leqslant n \\ 1 \leqslant\ell \leqslant m}}
\alpha_{k \ell} T_{k \ell}(v_{j}) = \sum_{1 \leqslant\ell \leqslant m}
\alpha_{j\ell} w_{\ell}.\]</span> So let us take <span class="math inline">\(\alpha_{k \ell} = \beta_{\ell k}\)</span> for each <span class="math inline">\(k\)</span> and <span class="math inline">\(\ell\)</span>. We then conclude that <span class="math inline">\(S\)</span> is the unique transformation satisfying <span class="math display">\[S(v_{j}) = \sum_{\ell = 1}^{m} \beta_{\ell j} w_{\ell} = T(v_{j})
\qquad \text{for $j = 1$, $2$, \ldots, $n$};\]</span> that is, <span class="math inline">\(S = T\)</span>. We conclude <span class="math display">\[T = \sum_{\substack{1 \leqslant k \leqslant n \\ 1 \leqslant\ell \leqslant m}}
\beta_{\ell k} T_{k \ell}\]</span> where <span class="math inline">\(B = \operatorname{Mat}_{\mathscr{B},\mathscr{C}}(T) = [\beta_{ij}]\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p> Let <span class="math inline">\(\mathscr{A}= \{\,T_{k\ell}\mid 1 \leqslant k \leqslant n, \; 1 \leqslant\ell \leqslant  m\,\}\)</span>. We have just shown that <span class="math inline">\(\mathscr{A}\)</span> is a spanning set for <span class="math inline">\(\mathcal{L}(V,W)\)</span>. Now consider an expression of linear dependence involving <span class="math inline">\(\mathscr{A}\)</span>, say <span class="math display">\[S = \sum_{\substack{1 \leqslant k \leqslant n \\ 1 \leqslant\ell \leqslant m}}
\alpha_{k\ell} T_{k\ell} = 0.\]</span> This asserts that <span class="math inline">\(S\)</span> is the zero transformation. In particular, <span class="math inline">\(S(v_{j}) = \vec{0}\)</span> for any <span class="math inline">\(j\)</span>. But, by definition, <span class="math display">\[S(v_{j}) = \sum_{\substack{1 \leqslant k \leqslant n \\ 1 \leqslant\ell \leqslant m}}
\alpha_{k\ell} T_{k\ell}(v_{j}) = \sum_{1 \leqslant\ell \leqslant m}
\alpha_{j\ell} w_{\ell}.\]</span> Thus <span class="math display">\[\sum_{\ell = 1}^{m} \alpha_{j\ell} w_{\ell} = \vec{0}\]</span> for <span class="math inline">\(j = 1\)</span>, <span class="math inline">\(2\)</span>, …, <span class="math inline">\(n\)</span>. Now <span class="math inline">\(\mathscr{C} = \{ w_{1},w_{2},\dots,w_{m} \}\)</span> is linearly independent, so we conclude <span class="math inline">\(\alpha_{j\ell} = 0\)</span> for all <span class="math inline">\(j\)</span> and all <span class="math inline">\(\ell\)</span>. Hence <span class="math inline">\(\mathscr{A}\)</span> is linearly independent.</p>
<p>We conclude that <span class="math inline">\(\mathscr{A}\)</span> is a basis for <span class="math inline">\(\mathcal{L}(V,W)\)</span>, so <span class="math display">\[\dim \mathcal{L}(V,W) = |\mathscr{A}| = nm = \dim V \cdot \dim W.\square\]</span></p></li>
</ol>
</div></li>
<li><p><span id="problem-04-08" label="problem-04-08"></span></p>
<div class="questionjupyter">
<p>Let <span class="math inline">\(T \colon \mathbb{R}^{2} \longrightarrow\mathbb{R}^{2}\)</span> be the linear mapping whose matrix with respect to the standard basis for <span class="math inline">\(\mathbb{R}^{2}\)</span> is <span class="math display">\[A = \begin{pmatrix}
4 &amp; 3 \\
1 &amp; 2
\end{pmatrix};\]</span> that is, <span class="math inline">\(T(\vec{v}) = A\vec{v}\)</span> for all <span class="math inline">\(\vec{v} \in \mathbb{R}^{2}\)</span>.</p>
<ol type="1">
<li><p>Show that <span class="math display">\[\mathscr{B} = \left\{ \begin{pmatrix}
 3 \\
 -1 
 \end{pmatrix}, \begin{pmatrix}
 -5 \\
 2 
 \end{pmatrix} \right\}\]</span> is a basis for <span class="math inline">\(\mathbb{R}^{2}\)</span>.</p></li>
<li><p>Calculate the matrix <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span> of <span class="math inline">\(T\)</span> with respect to the basis <span class="math inline">\(\mathscr{B}\)</span>.</p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong></p>
<ol type="1">
<li><p>We attempt to solve <span class="math display">\[\alpha \begin{pmatrix}3\\-1\end{pmatrix} + \beta \begin{pmatrix}-5\\2\end{pmatrix} = \begin{pmatrix}0\\0\end{pmatrix},\]</span> so <span class="math display">\[\begin{pmatrix}
3 &amp; -5 \\
-1 &amp; 2
\end{pmatrix}
\begin{pmatrix}\alpha\\\beta\end{pmatrix} = \begin{pmatrix}0\\0\end{pmatrix}.\]</span> Now <span class="math display">\[\det \begin{pmatrix}
3 &amp; -5 \\
-1 &amp; 2
\end{pmatrix}
= 6 - 5 = 1,\]</span> so <span class="math inline">\(A = \begin{pmatrix} 3 &amp; -5 \\ -1 &amp; 2 \end{pmatrix}\)</span> is invertible and now multiplying by its inverse yields <span class="math display">\[\begin{pmatrix}\alpha\\\beta\end{pmatrix} = A^{-1} \begin{pmatrix}0\\0\end{pmatrix} = \begin{pmatrix}0\\0\end{pmatrix}.\]</span> Hence <span class="math inline">\(\mathscr{B}= \left\{ \begin{pmatrix}3\\-1\end{pmatrix}, \begin{pmatrix}-5\\2\end{pmatrix} \right\}\)</span> is linearly independent and, since a basis for <span class="math inline">\(\mathbb{R}^{2}\)</span> is a linearly independent set of size <span class="math inline">\(2\)</span>, we conclude <span class="math inline">\(\mathscr{B}\)</span> is a basis for <span class="math inline">\(\mathbb{R}^{2}\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p>Let <span class="math inline">\(\mathscr{E} = \left\{ \begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}0\\1\end{pmatrix} \right\}\)</span> and let <span class="math inline">\(P\)</span> be the change of basis matrix corresponding to writing each vector in <span class="math inline">\(\mathscr{B}\)</span> as a linear combination of those in <span class="math inline">\(\mathscr{E}\)</span>. We have <span class="math display">\[\begin{aligned}
  \begin{pmatrix}3\\-1\end{pmatrix} = 3\begin{pmatrix}1\\0\end{pmatrix} - \begin{pmatrix}0\\1\end{pmatrix} \quad\text{and}\quad
\begin{pmatrix}-5\\2\end{pmatrix} = -5\begin{pmatrix}1\\0\end{pmatrix} + 2\begin{pmatrix}0\\1\end{pmatrix},\end{aligned}\]</span> so <span class="math display">\[P = \begin{pmatrix}
3 &amp; -5 \\
-1 &amp; 2
\end{pmatrix}.\]</span> Then <span class="math inline">\(\det P = 1\)</span>, so <span class="math display">\[P^{-1} = \begin{pmatrix}
2 &amp; 5 \\
1 &amp; 3
\end{pmatrix}\]</span> and therefore <span class="math display">\[\begin{aligned}
\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T) = P^{-1} \cdot \operatorname{Mat}_{\mathscr{E},\mathscr{E}}(T) \cdot
P 
&amp;= \begin{pmatrix} 2 &amp; 5 \\ 1 &amp; 3 \end{pmatrix}
\begin{pmatrix} 4 &amp; 3 \\ 1 &amp; 2 \end{pmatrix}
\begin{pmatrix} 3 &amp; -5 \\ -1 &amp; 2 \end{pmatrix} \\
&amp;= \begin{pmatrix} 13 &amp; 16 \\ 7 &amp; 9 \end{pmatrix}
\begin{pmatrix} 3 &amp; -5 \\ -1 &amp; 2 \end{pmatrix} \\
&amp;= \begin{pmatrix} 23 &amp; -33 \\ 12 &amp; -17 \end{pmatrix},\end{aligned}\]</span> which is the required answer.</p>
<p>Alternatively, one could just attempt to do this directly (and this will provide a check of the above calculation). We see <span class="math display">\[T\begin{pmatrix}3\\-1\end{pmatrix} = \begin{pmatrix} 4 &amp; 3 \\ 1 &amp; 2 \end{pmatrix}
\begin{pmatrix}3\\-1\end{pmatrix} = \begin{pmatrix}9\\1\end{pmatrix}\]</span> and now we must write <span class="math inline">\(\begin{pmatrix}9\\1\end{pmatrix}\)</span> as a linear combination of <span class="math inline">\(\begin{pmatrix}3\\-1\end{pmatrix}\)</span> and <span class="math inline">\(\begin{pmatrix}-5\\2\end{pmatrix}\)</span>. We solve <span class="math display">\[\alpha\begin{pmatrix}3\\-1\end{pmatrix} + \beta\begin{pmatrix}-5\\2\end{pmatrix} = \begin{pmatrix}9\\1\end{pmatrix},\]</span> so <span class="math display">\[\begin{array}{r@{}r@{}l}
3\alpha &amp;{} - 5\beta &amp;{} = 9 \\
-\alpha &amp;{} + 2\beta &amp;{} = 1.
\end{array}\]</span> Multiplying the second equation by <span class="math inline">\(3\)</span> and adding gives <span class="math inline">\(\beta = 12\)</span>. Hence <span class="math inline">\(3\alpha = 9+5\beta = 69\)</span>, so <span class="math inline">\(\alpha = 23\)</span>. Thus <span class="math display">\[T\begin{pmatrix}3\\-1\end{pmatrix} = 23\begin{pmatrix}3\\-1\end{pmatrix} + 12\begin{pmatrix}-5\\2\end{pmatrix}\]</span> and the first column of <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span> is therefore <span class="math display">\[\begin{matrix}
23 \\ 12
\end{matrix} \; .\]</span> Similarly <span class="math display">\[T\begin{pmatrix}-5\\2\end{pmatrix} = \begin{pmatrix} 4 &amp; 3 \\ 1 &amp; 2 \end{pmatrix}
\begin{pmatrix}-5\\2\end{pmatrix} = \begin{pmatrix}-14\\-1\end{pmatrix}\]</span> and we solve <span class="math display">\[\gamma \begin{pmatrix}3\\-1\end{pmatrix} + \delta \begin{pmatrix}-5\\2\end{pmatrix} = \begin{pmatrix}-14\\-1\end{pmatrix};\]</span> i.e., <span class="math display">\[\begin{array}{r@{}r@{}l}
3\gamma &amp;{} -5\delta &amp;{} = -14 \\
-\gamma &amp;{} + 2\delta &amp;{} = -1.
\end{array}\]</span> We deduce <span class="math inline">\(\delta = -17\)</span> and <span class="math inline">\(\gamma = -33\)</span>. Hence <span class="math display">\[\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T) = \begin{pmatrix}
23 &amp; -33 \\
12 &amp; -17
\end{pmatrix}\]</span> (which, of course, is the same answer as was obtained by use of the change of basis theorem).</p></li>
</ol>
</div></li>
<li><p><span id="problem-04-09" label="problem-04-09"></span></p>
<div class="questionjupyter">
<p>Define the linear transformation <span class="math inline">\(T \colon \mathbb{R}^{3} \longrightarrow\mathbb{R}^{3}\)</span> by <span class="math display">\[T\begin{pmatrix}
 x \\
 y \\
 z \\
 \end{pmatrix} = \begin{pmatrix}
 x+2y+2z \\
 -3x+4y-2z \\
 -2y \\
 \end{pmatrix}.\]</span></p>
<ol type="1">
<li><p>Find the matrix of <span class="math inline">\(T\)</span> with respect to the standard basis for <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p></li>
<li><p>Show that <span class="math display">\[\mathscr{B} = \left\{ \begin{pmatrix}
 1 \\
 1 \\
 1 \\
 \end{pmatrix}, \begin{pmatrix}
 1 \\
 2 \\
 2 \\
 \end{pmatrix},
  \begin{pmatrix}
 1 \\
 2 \\
 1 \\
 \end{pmatrix} \right\}\]</span> is a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p></li>
<li><p>Find the matrix of <span class="math inline">\(T\)</span> with respect to the basis <span class="math inline">\(\mathscr{B}\)</span>.</p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong></p>
<ol type="1">
<li><p>Let <span class="math inline">\(\mathscr{E} = \{ \vec{e}_{1},\vec{e}_{2},\vec{e}_{3} \}\)</span> denote the standard basis for <span class="math inline">\(\mathbb{R}^{3}\)</span>. Then <span class="math display">\[\begin{aligned}
T(\vec{e}_{1}) &amp;= T\begin{pmatrix}1\\0\\0\end{pmatrix} = \begin{pmatrix}1\\-3\\0\end{pmatrix} = \vec{e}_{1} -
3\vec{e}_{2} \\
T(\vec{e}_{2}) &amp;= T\begin{pmatrix}0\\1\\0\end{pmatrix} = \begin{pmatrix}2\\4\\-2\end{pmatrix} = 2\vec{e}_{1} +
4\vec{e}_{2} - 2\vec{e}_{3} \\
T(\vec{e}_{3}) &amp;= T\begin{pmatrix}0\\0\\1\end{pmatrix} = \begin{pmatrix}2\\-2\\0\end{pmatrix} = 2\vec{e}_{1} -
2\vec{e}_{2}.\end{aligned}\]</span> Hence the matrix of <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathscr{E}\)</span> is <span class="math display">\[\operatorname{Mat}_{\mathscr{E},\mathscr{E}}(T) = \begin{pmatrix}
1 &amp; 2 &amp; 2 \\
-3 &amp; 4 &amp; -2 \\
0 &amp; -2 &amp; 0
\end{pmatrix}.\square\]</span></p>
<div class="center">
<hr />
</div></li>
<li><p> We first show that <span class="math inline">\(\mathscr{B}\)</span> is linearly independent. Solve <span class="math display">\[\alpha\begin{pmatrix}1\\1\\1\end{pmatrix} + \beta\begin{pmatrix}1\\2\\2\end{pmatrix} +
\gamma\begin{pmatrix}1\\2\\1\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix};\]</span> that is, <span class="math display">\[\begin{pmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{pmatrix} \begin{pmatrix}\alpha\\\beta\\\gamma\end{pmatrix} = \begin{pmatrix}0\\0\\0\end{pmatrix}.\]</span> Apply the following row operations: <span class="math display">\[\begin{aligned}
\left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;&amp;\!\!\!\begin{array}{r@{}l}
r_{2} &amp;{} \mapsto r_{2}-r_{1} \\
r_{3} &amp;{} \mapsto r_{3}-r_{1}
\end{array} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;&amp;r_{2} \leftrightarrow r_{3} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{matrix} &amp;
\begin{matrix} 0 \\ 0 \\ 0 \end{matrix}
\end{array} \right)
&amp;&amp;r_{3} \mapsto r_{3}-r_{2}\end{aligned}\]</span> Hence the original equation is equivalent to <span class="math display">\[\begin{array}{r@{}r@{}r@{}l}
\alpha &amp;{} + \beta &amp;{} + \gamma &amp;{} = 0 \\
&amp;\beta &amp; &amp;{} = 0 \\
&amp; &amp; \gamma &amp;{} = 0.
\end{array}\]</span> So <span class="math inline">\(\alpha = \beta = \gamma = 0\)</span> and we deduce <span class="math inline">\(\mathscr{B}\)</span> is linearly independent. As <span class="math inline">\(\dim \mathbb{R}^{3} = 3\)</span>, we conclude <span class="math inline">\(\mathscr{B}\)</span> is indeed a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p> Let <span class="math inline">\(P\)</span> denote the change of basis matrix corresponding to writing each vector in <span class="math inline">\(\mathscr{B}\)</span> in terms of those in <span class="math inline">\(\mathscr{E}\)</span>. Since <span class="math display">\[\begin{aligned}
\begin{pmatrix}1\\1\\1\end{pmatrix} &amp;= \vec{e}_{1} + \vec{e}_{2} + \vec{e}_{3} \\
\begin{pmatrix}1\\2\\2\end{pmatrix} &amp;= \vec{e}_{1} + 2\vec{e}_{2} + 2\vec{e}_{3} \\
\begin{pmatrix}1\\2\\1\end{pmatrix} &amp;= \vec{e}_{1} + 2\vec{e}_{2} + \vec{e}_{3},\end{aligned}\]</span> we have <span class="math display">\[P = \begin{pmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{pmatrix}.\]</span> To calculate the inverse, we again perform row operations: <span class="math display">\[\begin{aligned}
\left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{matrix} &amp;
\begin{matrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{matrix} \end{array} \right)
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{matrix} &amp;
\begin{matrix}
1 &amp; 0 &amp; 0 \\
-1 &amp; 1 &amp; 0 \\
-1 &amp; 0 &amp; 1
\end{matrix} \end{array} \right)
&amp;&amp;\!\!\!\begin{array}{r@{}l}
r_{2} &amp;{} \mapsto r_{2}-r_{1} \\
r_{3} &amp;{} \mapsto r_{3}-r_{1}
\end{array} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{matrix} &amp;
\begin{matrix}
1 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 1 \\
-1 &amp; 1 &amp; 0
\end{matrix}
\end{array} \right)
&amp;&amp;r_{2} \leftrightarrow r_{3} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{matrix} &amp;
\begin{matrix}
2 &amp; 0 &amp; -1 \\
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -1
\end{matrix}
\end{array} \right)
&amp;&amp;\!\!\!\begin{array}{l@{}r}
r_{1} &amp;{} \mapsto r_{1}-r_{2} \\
r_{3} &amp;{} \mapsto r_{3}-r_{2}
\end{array} \\
%%%%%%%%%%%%%%%%%%%%
&amp;\longrightarrow \left( \begin{array}{c|c}
\begin{matrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{matrix} &amp;
\begin{matrix}
2 &amp; -1 &amp; 0 \\
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -1
\end{matrix}
\end{array} \right)
&amp;&amp;r_{1} \mapsto r_{1}-r_{3}\end{aligned}\]</span> Hence <span class="math display">\[P^{-1} = \begin{pmatrix}
2 &amp; -1 &amp; 0 \\
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -1
\end{pmatrix}.\]</span> Therefore the matrix of <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathscr{B}\)</span> is <span class="math display">\[\begin{aligned}
\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T) &amp;= P^{-1} \cdot \operatorname{Mat}_{\mathscr{E},\mathscr{E}}(T) \cdot
P \\
&amp;= \begin{pmatrix}
2 &amp; -1 &amp; 0 \\
-1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; -1
\end{pmatrix}
\begin{pmatrix}
1 &amp; 2 &amp; 2 \\
-3 &amp; 4 &amp; -2 \\
0 &amp; -2 &amp; 0
\end{pmatrix}
\begin{pmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{pmatrix} \\
&amp;= \begin{pmatrix}
5 &amp; 0 &amp; 6 \\
-1 &amp; -4 &amp; -2 \\
-3 &amp; 6 &amp; -2
\end{pmatrix}
\begin{pmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{pmatrix} \\
&amp;= \begin{pmatrix}
11 &amp; 17 &amp; 11 \\
-7 &amp; -13 &amp; -11 \\
1 &amp; 5 &amp; 7
\end{pmatrix}.\end{aligned}\]</span></p>
<p>We can check this answer is correct as follows: <span class="math display">\[\begin{aligned}
T\begin{pmatrix}1\\1\\1\end{pmatrix} &amp;= \begin{pmatrix}5\\-1\\-2\end{pmatrix} = 11\begin{pmatrix}1\\1\\1\end{pmatrix}
- 7\begin{pmatrix}1\\2\\2\end{pmatrix} + \begin{pmatrix}1\\2\\1\end{pmatrix} \\
T\begin{pmatrix}1\\2\\2\end{pmatrix} &amp;= \begin{pmatrix}9\\1\\-4\end{pmatrix} = 17\begin{pmatrix}1\\1\\1\end{pmatrix} -
13\begin{pmatrix}1\\2\\2\end{pmatrix} + 5\begin{pmatrix}1\\2\\1\end{pmatrix} \\
T\begin{pmatrix}1\\2\\1\end{pmatrix} &amp;= \begin{pmatrix}7\\3\\-4\end{pmatrix} = 11\begin{pmatrix}1\\1\\1\end{pmatrix} -
11\begin{pmatrix}1\\2\\2\end{pmatrix} + 7\begin{pmatrix}1\\2\\1\end{pmatrix}\end{aligned}\]</span> This confirms our calculation above is correct.</p></li>
</ol>
</div></li>
<li><p><span id="problem-04-10" label="problem-04-10"></span></p>
<div class="questionjupyter">
<p>A linear transformation <span class="math inline">\(T \colon V \longrightarrow V\)</span> is said to be <strong><em>nilpotent</em></strong> of <em>index <span class="math inline">\(k\)</span></em> if <span class="math inline">\(T^{k}\)</span> is the zero map but <span class="math inline">\(T^{k-1}\)</span> is not.</p>
<ol type="1">
<li><p>Suppose that <span class="math inline">\(V\)</span> is a vector space of dimension <span class="math inline">\(n\)</span> and the linear transformation <span class="math inline">\(T \colon V \longrightarrow V\)</span> is nilpotent of index <span class="math inline">\(n\)</span>. Choose a vector <span class="math inline">\(v\)</span> such that <span class="math inline">\(T^{n-1}(v) \neq \vec{0}\)</span>. Show that <span class="math display">\[\mathscr{B} = \{ v, T(v), T^{2}(v), \dots, T^{n-1}(v) \}\]</span> is a basis for <span class="math inline">\(V\)</span>. [Hint: Show that it is linearly independent. Consider an expression of the form <span class="math inline">\(\sum_{i=0}^{n-1} \alpha_{i}  T^{i}(v) = \vec{0}\)</span> and apply a suitable power of <span class="math inline">\(T\)</span>.]</p></li>
<li><p>Write down the matrix of <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathscr{B}\)</span>.</p></li>
<li><p>Suppose <span class="math inline">\(T \colon \mathbb{R}^{3} \longrightarrow\mathbb{R}^{3}\)</span> is a linear transformation whose matrix with respect to the standard basis for <span class="math inline">\(\mathbb{R} ^ 3\)</span> is <span class="math display">\[A = \begin{pmatrix}
          0 &amp; 0 &amp; 2 \\
          1 &amp; -1 &amp; 1 \\
          -1 &amp; 1 &amp; 1
        \end{pmatrix}.\]</span> Show that <span class="math inline">\(T\)</span> is nilpotent of index <span class="math inline">\(3\)</span>. Find a basis with respect to which <span class="math inline">\(T\)</span> has matrix <span class="math display">\[\begin{pmatrix}
          0 &amp; 0 &amp; 0 \\
          1 &amp; 0 &amp; 0 \\
          0 &amp; 1 &amp; 0
        \end{pmatrix}.\]</span></p></li>
</ol>
</div>
<button type="button" class="collapsible">
<p>SOLUTION.</p>
</button>
<div class="solution04">
<ol type="1">
<li><p>Let <span class="math inline">\(T \colon V \longrightarrow V\)</span> be nilpotent of index <span class="math inline">\(n\)</span>. Suppose that <span class="math display">\[\mathscr{B}= \{ v, T(v), T^{2}(v), \dots, T^{n-1}(v) \}\]</span> is linearly dependent. Then the equation <span class="math display">\[\sum_{i=1}^{n} \alpha_{i} T^{i-1}(v) = \vec{0}\]</span> has a non-zero solution. Suppose that <span class="math inline">\(\alpha_{j}\)</span> is the first non-zero coefficient (i.e., <span class="math inline">\(\alpha_{1} = \dots = \alpha_{j-1} = 0\)</span> and <span class="math inline">\(\alpha_{j} \neq 0\)</span>). Then <span class="math display">\[\sum_{i=j}^{n} \alpha_{i} T^{i-1}(v) = \vec{0}.\]</span> Apply <span class="math inline">\(T^{n-j}\)</span>: <span class="math display">\[T^{n-j} \biggl( \sum_{i=j}^{n} \alpha_{i} T^{i-1}(v) \biggr) =
T^{n-j}(\vec{0}) =  \vec{0}.\]</span> Linearity then gives <span class="math display">\[\alpha_{j} T^{n-1}(v) + \alpha_{j+1} T^{n}(v) + \dots + \alpha_{n}
T^{2n-j-1}(v) = \vec{0}.\]</span> Since <span class="math inline">\(T^{n} = 0\)</span>, we conclude <span class="math display">\[T^{n}(v) = T^{n+1}(v) = \dots = T^{2n-j-1}(v) = \vec{0}\]</span> (note <span class="math inline">\(T^{n+r}(v) = T^{r}(T^{n}(v)) = T^{r}(\vec{0}) = \vec{0}\)</span> for all <span class="math inline">\(r \geqslant 0\)</span>). Hence <span class="math display">\[\alpha_{j} \, T^{n-1}(v) = \vec{0}\]</span> but we assumed <span class="math inline">\(\alpha_{j} \neq 0\)</span> and <span class="math inline">\(T^{n-1}(v) \neq \vec{0}\)</span>, which is a contradiction. Hence <span class="math inline">\(\mathscr{B}\)</span> is linearly independent. As <span class="math inline">\(\dim V = n\)</span>, our set <span class="math inline">\(\mathscr{B}\)</span> is indeed a basis for <span class="math inline">\(V\)</span>.</p>
<div class="center">
<hr />
</div></li>
<li><p>Let <span class="math inline">\(v_{1} = v\)</span>,  <span class="math inline">\(v_{2} = T(v)\)</span>, …, <span class="math inline">\(v_{n} = T^{n-1}(v)\)</span>. Then <span class="math display">\[T(v_{1}) = v_{2}, \qquad T(v_{2}) = v_{3}, \qquad \dots, \qquad
T(v_{n-1}) = v_{n}\]</span> (by definition) and <span class="math display">\[T(v_{n}) = T^{n}(v) = \vec{0}.\]</span></p>
<p>This specifies for us the coefficients of the matrix of <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathscr{B}\)</span>: <span class="math display">\[\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T) = \begin{pmatrix}
  0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\
  1 &amp; 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\ 
  0 &amp; 0 &amp; 1 &amp; \cdots &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\ 
  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp;\cdots &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp;\cdots &amp; \cdots &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp;\cdots &amp; \cdots &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}. \square\]</span></p>
<div class="center">
<hr />
</div></li>
<li><p><strong>This solution is here for reference only, you should use a computer to solve this type of problem in this course!</strong> Now consider the linear transformation <span class="math inline">\(T \colon \mathbb{R}^{3} \longrightarrow\mathbb{R}^{3}\)</span> whose matrix with respect to the standard basis is <span class="math display">\[A = \begin{pmatrix}
0 &amp; 0 &amp; 2 \\
1 &amp; -1 &amp; 1 \\
-1 &amp; 1 &amp; 1
\end{pmatrix},\]</span> so <span class="math display">\[T(\vec{v}) = A\vec{v} \qquad \text{for $\vec{v} \in \mathbb{R}^{3}$}.\]</span> We calculate <span class="math display">\[\begin{aligned}
A^{2} = \begin{pmatrix}
0 &amp; 0 &amp; 2 \\
1 &amp; -1 &amp; 1 \\
-1 &amp; 1 &amp; 1
\end{pmatrix}
\begin{pmatrix}
0 &amp; 0 &amp; 2 \\
1 &amp; -1 &amp; 1 \\
-1 &amp; 1 &amp; 1
\end{pmatrix} 
= \begin{pmatrix}
-2 &amp; 2 &amp; 2 \\
-2 &amp; 2 &amp; 2 \\
0 &amp; 0 &amp; 0
\end{pmatrix} \neq 0\end{aligned}\]</span> and <span class="math display">\[\begin{aligned}
A^{3} = \begin{pmatrix}
0 &amp; 0 &amp; 2 \\
1 &amp; -1 &amp; 1 \\
-1 &amp; 1 &amp; 1
\end{pmatrix}
\begin{pmatrix}
-2 &amp; 2 &amp; 2 \\
-2 &amp; 2 &amp; 2 \\
0 &amp; 0 &amp; 0
\end{pmatrix} 
= \begin{pmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{pmatrix} = 0.\end{aligned}\]</span> Hence <span class="math display">\[T^{2}(\vec{v}) = A^{2}\vec{v} \quad \text{is not the zero map}\]</span> but <span class="math display">\[T^{3}(\vec{v}) = A^{3}\vec{v} = 0\vec{v} = \vec{0}.\]</span> So <span class="math inline">\(T\)</span> is nilpotent of index <span class="math inline">\(3\)</span>.</p>
<p>Take <span class="math inline">\(\vec{v} = \begin{pmatrix}1\\0\\0\end{pmatrix}\)</span>. Then <span class="math display">\[T^{2}(\vec{v}) = A^{2}\vec{v} =
\begin{pmatrix}
-2 &amp; 2 &amp; 2 \\
-2 &amp; 2 &amp; 2 \\
0 &amp; 0 &amp; 0
\end{pmatrix}
\begin{pmatrix}1\\0\\0\end{pmatrix}
= \begin{pmatrix}-2\\-2\\0\end{pmatrix} \neq \vec{0}.\]</span> Hence <span class="math inline">\(\vec{v}\)</span> is a suitable vector to apply the first part of the question. Then <span class="math inline">\(\mathscr{B}= \{ \vec{v}, T(\vec{v}), T^{2}(\vec{v}) \}\)</span> is a basis for <span class="math inline">\(\mathbb{R}^{3}\)</span> and the matrix of <span class="math inline">\(T\)</span> is <span class="math display">\[\begin{pmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{pmatrix}\]</span> with respect to this basis. We have already calculated <span class="math inline">\(T^{2}(\vec{v})\)</span> and finally <span class="math display">\[T(\vec{v}) = A\vec{v} =
\begin{pmatrix}
0 &amp; 0 &amp; 2 \\
1 &amp; -1 &amp; 1 \\
-1 &amp; 1 &amp; 1
\end{pmatrix}
\begin{pmatrix}1\\0\\0\end{pmatrix} = \begin{pmatrix}0\\1\\-1\end{pmatrix}.\]</span> Hence with respect to <span class="math display">\[\mathscr{B}= \left\{ \begin{pmatrix}1\\0\\0\end{pmatrix}, \begin{pmatrix}0\\1\\-1\end{pmatrix},
  \begin{pmatrix}-2\\-2\\0\end{pmatrix} \right\}\]</span> the matrix of <span class="math inline">\(T\)</span> has the required form.</p></li>
</ol>
</div></li>
</ol>







<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>

<p><a href="#">Back to top</a></p>
<ul>
    
    <li>
      <a href="/mt3501-lnotes/">
      
      Home
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/problems/">
      
      Problems
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/01-intro/">
      
      Section 1 - Intro
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/02-vector-spaces/">
      
      Section 2 - Vector spaces
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/03-algorithms-to-live-by/">
      
      Section 3 - Algorithms to live by
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/04-linear-transf/">
      
      Section 4 - Linear transformations
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/05-l-v-w/">
      
      Section 5 - The vector space of linear transformations
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/06-direct-sums/">
      
      Section 6 - Direct sums
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/07-dual-space/">
      
      Section 7 - The dual space
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/08-eigen-stuff/">
      
      Section 8 - Eigenvectors, eigenvalues, and the characteristic polynomial
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/09-diagonal/">
      
      Section 9 - Diagonalisation
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/10-jnf/">
      
      Section 10 - Jordan normal form
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/11-inner-products/">
      
      Section 11 - Inner products
      </a>
    </li>
    
</ul>
<footer>
<hr>⚡️
	2021  © J. D. Mitchell  
</footer>
</body>
</html>
