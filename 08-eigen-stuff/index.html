<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
   "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en-uk" xmlns="http://www.w3.org/1999/xhtml"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="data:;base64,iVBORw0KGgo=" />
    <link rel="stylesheet" href="../css/math.css">
    
    
    <title>MT3501 Lecture Notes | </title>
    <style type="text/css">
  body {
    font-size: 150%;
    font-family: muli,avenir,helvetica neue,helvetica,ubuntu,roboto,noto,segoe ui,arial,sans-serif;
  }
</style>

</head>
<body><p><a name="nav-menu" id="nav-menu"><strong>Contents</strong></a></p>

<ul>
    
    <li>
      <a href="/mt3501-lnotes/">
      
      Home
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/01-intro/">
      
      Section 1 - Intro
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/02-vector-spaces/">
      
      Section 2 - Vector spaces
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/03-algorithms-to-live-by/">
      
      Section 3 - Algorithms to live by
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/04-linear-transf/">
      
      Section 4 - Linear transformations
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/05-l-v-w/">
      
      Section 5 - The vector space of linear transformations
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/06-direct-sums/">
      
      Section 6 - Direct sums
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/07-dual-space/">
      
      Section 7 - The dual space
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/08-eigen-stuff/">
      
      Section 8 - Eigenvectors, eigenvalues, and the characteristic polynomial
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/09-diagonal/">
      
      Section 9 - Diagonalisation
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/10-jnf/">
      
      Section 10 - Jordan normal form
      </a>
    </li>
    
    <li>
      <a href="/mt3501-lnotes/11-inner-products/">
      
      Section 11 - Inner products
      </a>
    </li>
    
</ul>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    





  </p>






<h1 id="eigenvectors-eigenvalues-and-the-characteristic-polynomial">Eigenvectors, eigenvalues, and the characteristic polynomial</h1>
<style type="text/css" scoped>
  body {
    counter-reset: chapter 7;
  }
</style>

<h2 id="eigenvectors-and-eigenvalues">Eigenvectors and eigenvalues</h2>
<div class="defn">
<p><span id="defn-eigen" label="defn-eigen"></span> Let <span class="math inline">\(V\)</span> be a vector space over a field <span class="math inline">\(F\)</span> and let <span class="math inline">\(T : V \to V\)</span> be a linear transformation. A <em>non-zero</em> vector <span class="math inline">\(v\)</span> is an <strong><em>eigenvector</em></strong> for <span class="math inline">\(T\)</span> with <em>eigenvalue</em> <span class="math inline">\(\lambda \in  F\)</span> if <span class="math display">\[T(v) = \lambda v.\]</span></p>
</div>
<p>Note that <span class="math inline">\(T(\vec{0}) = \vec{0} = \lambda\vec{0}\)</span> for every <span class="math inline">\(\lambda \in F\)</span>, which is why <span class="math inline">\(v\)</span> is non-zero in <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#defn-eigen">Definition 8.1.1</a>.</p>
<div class="defn">
<p>Let <span class="math inline">\(V\)</span> be a vector space over a field <span class="math inline">\(F\)</span>, let <span class="math inline">\(T : V \to V\)</span> be a linear transformation, and <span class="math inline">\(\lambda \in F\)</span>. The <strong><em>eigenspace</em></strong> corresponding to the eigenvalue <span class="math inline">\(\lambda\)</span> is the subspace <span class="math display">\[\begin{aligned}
    E_{\lambda} = \ker(T-\lambda \operatorname{id}) 
    &amp; = \{v \in V : T(v) - \lambda v = \vec{0}\}                                                                \\
                                    &amp; = \{v \in V : T(v) = \lambda v\},
  \end{aligned}\]</span> where <span class="math inline">\(\operatorname{id}: V \to V\)</span> is the identity transformation. In other words, the eigenspace of an eigenvalue <span class="math inline">\(\lambda\)</span> is the subspace of <span class="math inline">\(V\)</span> consisting of all of the eigenvectors for <span class="math inline">\(\lambda\)</span> and the zero vector <span class="math inline">\(\vec{0}_V\)</span>.</p>
</div>
<p>Note that <span class="math inline">\(T-\lambda I\)</span> is a linear transformation and so <span class="math inline">\(E_{\lambda}=\ker(T-\lambda I)\)</span> is a subspace of <span class="math inline">\(V\)</span> by <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#prop-image-kernel-subspaces">Proposition 4.1.5</a>.</p>
<h2 id="characteristic-polynomial">Characteristic polynomial</h2>
<div class="defn">
<p>Let <span class="math inline">\(T : V \to V\)</span> be a linear transformation of the finite-dimensional vector space <span class="math inline">\(V\)</span> (over <span class="math inline">\(F\)</span>) and let <span class="math inline">\(A\)</span> be the matrix of <span class="math inline">\(T\)</span> with respect to some basis. The <strong><em>characteristic polynomial</em></strong> of <span class="math inline">\(T\)</span> is <span class="math display">\[c_{T}(x) = \det(xI-A)\]</span> where <span class="math inline">\(x\)</span> is an indeterminate variable.</p>
</div>
<div class="prop">
<p><span id="prop-eigenvalue-root" label="prop-eigenvalue-root"></span> Suppose that <span class="math inline">\(T : V \to V\)</span> is a linear transformation of the finite-dimensional vector space <span class="math inline">\(V\)</span> over <span class="math inline">\(F\)</span>. Then <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(T\)</span> if and only if <span class="math inline">\(\lambda\)</span> is a root of the characteristic polynomial of <span class="math inline">\(T\)</span>.</p>
</div>
<div class="cor">
<p>Suppose that <span class="math inline">\(T : V \to V\)</span> is a linear transformation of the finite-dimensional vector space <span class="math inline">\(V\)</span> over <span class="math inline">\(\mathbb{C}\)</span>. Then <span class="math inline">\(T\)</span> has at least one eigenvalue <span class="math inline">\(\lambda \in \mathbb{C}\)</span>.</p>
</div>
<p>The characteristic polynomial of a linear transformation <span class="math inline">\(T:V \to V\)</span> is defined in terms of a matrix <span class="math inline">\(A\)</span> for <span class="math inline">\(T\)</span>, and the matrix depends on a particular choice of bases for <span class="math inline">\(V\)</span>. Hence it appears that the characteristic polynomial of <span class="math inline">\(T\)</span> depends on the choice of bases for <span class="math inline">\(V\)</span>.</p>
<div class="prop">
<p><span id="prop-char-poly-indep" label="prop-char-poly-indep"></span> Let <span class="math inline">\(V\)</span> be a finite-dimensional vector space <span class="math inline">\(V\)</span> over <span class="math inline">\(F\)</span> and <span class="math inline">\(T : V \to V\)</span> be a linear transformation. The characteristic polynomial <span class="math inline">\(c_{T}(x)\)</span> is independent of the choice of basis for <span class="math inline">\(V\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be matrices for <span class="math inline">\(T\)</span> with respect to some bases for <span class="math inline">\(V\)</span>. <a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-change-basis">Theorem 4.3.1</a> tells us that <span class="math inline">\(B = P^{-1}AP\)</span> for some invertible matrix <span class="math inline">\(P\)</span>. Then <span class="math display">\[P^{-1} (xI-A) P = xP^{-1}IP - P^{-1}AP = xI - B ,\]</span> so <span class="math display">\[\begin{aligned}
    \det(xI - B) &amp; = \det(P^{-1}(xI-A)P)                         \\
                 &amp; = \det P^{-1} \cdot \det(xI-A) \cdot \det P   \\
                 &amp; = (\det P)^{-1} \cdot \det(xI-A) \cdot \det P \\
                 &amp; = \det(xI-A)
  \end{aligned}\]</span> (since multiplication in the field <span class="math inline">\(F\)</span> is commutative — see condition (5) of <a href="https://jdbm.me/mt3501-lnotes/02-vector-spaces/#def-field">Definition 2.1.1</a>). ◻</p>
</div>
<h2 class="unnumbered" id="eigenvalue-magic-tricks">Eigenvalue magic tricks</h2>
<p>In this section, we present some tricks for very rapidly computing the eigenvalues of a linear transformation, or matrix. These tricks are primarily intended for when you might want to find the eigenvalues of a linear transformation by hand.</p>
<p>Throughout this section we suppose that <span class="math inline">\(T : V\to V\)</span> is a linear transformation from an <span class="math inline">\(n\)</span>-dimensional vector space <span class="math inline">\(V\)</span> to itself, and that <span class="math inline">\(A\)</span> is the matrix of <span class="math inline">\(T\)</span> with respect to some bases for <span class="math inline">\(V\)</span>.</p>
<p>Here are some definitions.</p>
<div class="defn">
<p>If <span class="math inline">\(A = [\alpha_{ij}]\)</span> is an <span class="math inline">\(n\times n\)</span> matrix over a field, then the <strong><em>trace</em></strong> of <span class="math inline">\(A\)</span> is the sum of the diagonal entries and it is denoted <span class="math inline">\(\operatorname{tr}(A)\)</span>. In other words, <span class="math inline">\(\operatorname{tr}(A) =  \sum_{i = 1} ^ n \alpha_{ii}\)</span>.</p>
</div>
<p>Tricks:</p>
<ol type="1">
<li><p>If <span class="math inline">\(V\)</span> is an <span class="math inline">\(n\)</span>-dimensional vector space, then <span class="math inline">\(T\)</span> has at most <span class="math inline">\(n\)</span> eigenvalues (not necessarily distinct).</p></li>
<li><p>If <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n\)</span> are the eigenvalues of <span class="math inline">\(T\)</span>, then <span class="math inline">\(\operatorname{tr}(A) = \lambda_1 + \cdots + \lambda_n\)</span>.</p></li>
<li><p>If <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n\)</span> are the eigenvalues of <span class="math inline">\(T\)</span>, then <span class="math inline">\(\det(A) = \lambda_1 \times \cdots \times \lambda_n\)</span>.</p></li>
<li><p>The eigenvalues of <span class="math inline">\(T\)</span> and its dual transformation <span class="math inline">\(T ^ *\)</span> are the same.</p></li>
<li><p>If for each row of <span class="math inline">\(A\)</span>, the sum of the entries is <span class="math inline">\(k\)</span>, then <span class="math inline">\(T\)</span> has an eigenvalue equal to <span class="math inline">\(k\)</span>.</p></li>
<li><p>If for each column of <span class="math inline">\(A\)</span>, the sum of the entries is <span class="math inline">\(k\)</span>, then <span class="math inline">\(T\)</span> has an eigenvalue equal to <span class="math inline">\(k\)</span>.</p></li>
<li><p>If <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n\)</span> are the eigenvalues of <span class="math inline">\(T\)</span>, then <span class="math inline">\(\lambda_1 ^ 2\)</span>, <span class="math inline">\(\lambda_2 ^ 2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n ^ 2\)</span> are the eigenvalues of <span class="math inline">\(T \circ T\)</span>.</p></li>
<li><p>If <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n\)</span> are the eigenvalues of <span class="math inline">\(T\)</span>, then the eigenvalues of <span class="math inline">\(T + T\)</span> are <span class="math inline">\(2\lambda_1\)</span>, <span class="math inline">\(2\lambda_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(2\lambda_n\)</span>.</p></li>
<li><p>If <span class="math inline">\(T(v) = \vec{0}\)</span> for all <span class="math inline">\(v\in V\)</span>, then <span class="math inline">\(\lambda_1 = \lambda_2 = \cdots  = \lambda_n = 0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\lambda_1\)</span>, <span class="math inline">\(\lambda_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n\)</span> are the eigenvalues of <span class="math inline">\(T\)</span>, then <span class="math inline">\(\lambda_1 + 2\)</span>, <span class="math inline">\(\lambda_2 + 2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\lambda_n + 2\)</span> are the eigenvalues of <span class="math inline">\(T + 2I\)</span>.</p></li>
</ol>
<div class="example">
<p><span id="ex-no-eigenvalues" label="ex-no-eigenvalues"></span> Suppose that <span class="math inline">\(T: \mathbb{R} ^ 2 \to \mathbb{R} ^ 2\)</span> is a linear transformation with matrix <span class="math display">\[A =
    \begin{pmatrix}
      1 &amp; -2 \\
      2 &amp; 0
    \end{pmatrix}.\]</span> Show that <span class="math inline">\(T\)</span> has no eigenvalues.</p>
</div>
<div class="solution">
<p>By <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#prop-eigenvalue-root">Proposition 8.2.2</a>, <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(T\)</span> if and only if <span class="math inline">\(\lambda\)</span> is a root of the characteristic polynomial <span class="math inline">\(c_T(x) =  c_A(x)\)</span>. Hence it suffices to show that <span class="math inline">\(c_A(x)\)</span> has no roots in <span class="math inline">\(\mathbb{R}\)</span>. By definition, <span class="math display">\[c_A(x) = \det(xI - A)
    =
    \det
    \begin{pmatrix}
      x - 1 &amp; 2 \\
      -2    &amp; x
    \end{pmatrix}
    = (x-1)x + 4
    = x ^ 2 - x + 4\]</span> and so by the quadratic formula, the roots of <span class="math inline">\(c_A(x)\)</span> are: <span class="math display">\[\frac{1 \pm \sqrt{-15}}{2}\not\in \mathbb{R}.\]</span> Hence <span class="math inline">\(c_A(x)\)</span> has no roots in <span class="math inline">\(\mathbb{R}\)</span>, and hence <span class="math inline">\(T\)</span> has no eigenvalues.</p>
</div>
<div class="example">
<p><span id="ex-trick-II-III" label="ex-trick-II-III"></span> Find the eigenvalues of the matrix over <span class="math inline">\(\mathbb{C}\)</span>: <span class="math display">\[A =
    \begin{pmatrix}
      1 &amp; 0 &amp; 1 \\
      0 &amp; 2 &amp; 1 \\
      1 &amp; 0 &amp; 3
    \end{pmatrix}.\]</span></p>
</div>
<div class="solution">
<p>Since <span class="math inline">\(A\)</span> is a matrix over <span class="math inline">\(\mathbb{C}\)</span> we know that <span class="math inline">\(A\)</span> has exactly <span class="math inline">\(3\)</span> eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \lambda_3\)</span>. By Tricks (2) and (3): <span class="math display">\[\lambda_1 + \lambda_2 + \lambda_3 = \operatorname{tr}(A) = 1 + 2 + 3 = 6\]</span> and <span class="math display">\[\lambda_1 \lambda_2 \lambda_3 = \det(A) = 4.\]</span> By inspection, <span class="math display">\[\begin{pmatrix}
      1 &amp; 0 &amp; 1 \\
      0 &amp; 2 &amp; 1 \\
      1 &amp; 0 &amp; 3
    \end{pmatrix}
    \begin{pmatrix} x \\ y \\ z \\ \end{pmatrix}
    =
    \begin{pmatrix} x + z \\ 2y + z \\ x + 3z \\ \end{pmatrix}
    =
    \lambda_1\begin{pmatrix} x \\ y \\ z \\ \end{pmatrix}\]</span> has <span class="math inline">\(\lambda_1 = 2\)</span> as a solution. Therefore <span class="math inline">\(\lambda_2 + \lambda_3 = 4\)</span> and <span class="math inline">\(\lambda_2\lambda_3 = 2\)</span>. Solving for <span class="math inline">\(\lambda_3\)</span> we obtain: <span class="math display">\[\lambda_3 ^ 2 - 4\lambda_3 + 2 = 0\]</span> and so <span class="math display">\[\lambda_2 = 2 + \sqrt{2}\quad\text{and}\quad
    \lambda_3 = 2 - \sqrt{2}.\]</span></p>
</div>
<div class="example">
<p>Find the eigenvalues of the matrix over <span class="math inline">\(\mathbb{C}\)</span>: <span class="math display">\[A =
    \begin{pmatrix}
      2 &amp; 0 &amp; 4  \\
      1 &amp; 4 &amp; 5  \\
      4 &amp; 0 &amp; 10
    \end{pmatrix}.\]</span></p>
</div>
<div class="solution">
<p>Note that <span class="math display">\[A =
    \begin{pmatrix}
      1 &amp; 0 &amp; 1 \\
      0 &amp; 2 &amp; 1 \\
      1 &amp; 0 &amp; 3
    \end{pmatrix}
    \begin{pmatrix}
      1 &amp; 0 &amp; 1 \\
      0 &amp; 2 &amp; 1 \\
      1 &amp; 0 &amp; 3
    \end{pmatrix}\]</span> and so, by Trick (7), the eigenvalues of <span class="math inline">\(A\)</span> are just the squares of the eigenvalues of <span class="math display">\[\begin{pmatrix}
      1 &amp; 0 &amp; 1 \\
      0 &amp; 2 &amp; 1 \\
      1 &amp; 0 &amp; 3
    \end{pmatrix}.\]</span> We calculated these in <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#ex-trick-II-III">Example 8.2.7</a>, and so the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(4\)</span>, <span class="math inline">\((2 + \sqrt{2}) ^ 2\)</span> and <span class="math inline">\((2 - \sqrt{2}) ^ 2\)</span>.</p>
</div>
<div class="example">
<p>Find the eigenvalue of the matrix over <span class="math inline">\(\mathbb{C}\)</span>: <span class="math display">\[A =
    \begin{pmatrix}
      1 &amp; 0 &amp; 2  \\
      0 &amp; 2 &amp; 1  \\
      1 &amp; 0 &amp; -1
    \end{pmatrix}.\]</span></p>
</div>
<div class="solution">
<p>Note that the column sums of <span class="math inline">\(A\)</span> are all <span class="math inline">\(2\)</span> and so we known that <span class="math inline">\(2\)</span> is an eigenvalue of <span class="math inline">\(A\)</span> by Trick (6). Also, if <span class="math inline">\(\lambda_2\)</span> and <span class="math inline">\(\lambda_3\)</span> are the other eigenvalues of <span class="math inline">\(A\)</span>, then <span class="math display">\[2 + \lambda_2 + \lambda_3 = 2\quad\text{and}\quad 2\lambda_2\lambda_3 =
    \det(A) = -6.\]</span> Hence, solving the second equation for <span class="math inline">\(\lambda_3\)</span>, we get <span class="math inline">\(\lambda_3 = \frac{-3}{\lambda_2}\)</span>, and so, by the first equation, <span class="math display">\[2 + \lambda_2 - \frac{3}{\lambda_2} = 2\]</span> solving for <span class="math inline">\(\lambda_2\)</span> yields <span class="math inline">\(\lambda_2 = \sqrt{3}\)</span> and <span class="math inline">\(\lambda_3 = -3/  \sqrt{3} = -\sqrt{3}\)</span>, as required.</p>
</div>
<h2 id="upper-triangular-matrices">Upper triangular matrices</h2>
<p>Two of the important themes in linear algebra are: finding “nice” matrices for a linear transformation; and decomposing matrices into products of “nice” matrices. In this section we consider the first (of at least three) type of “nice” matrices for a linear transformation. What’s the definition of “nice”? Well, there’s no mathematical definition, but if we are able to determine the answer to many questions by just looking at the matrix and performing no further calculations, then perhaps we can all agree that matrix is “nice”.</p>
<div class="defn">
<p>A matrix <span class="math inline">\(A = [\alpha_{ij}]\)</span> with entries in a field <span class="math inline">\(F\)</span> is <strong><em>upper triangular</em></strong> if <span class="math inline">\(\alpha_{ij} = 0\)</span> for all <span class="math inline">\(i &gt; j\)</span>. An upper triangular matrix is of the form <span class="math display">\[\begin{pmatrix}
    \alpha_{11} &amp; \alpha_{12} &amp; \cdots &amp; \alpha_{1n} \\
    0 &amp; \alpha_{22} &amp; \cdots &amp; \alpha_{2n} \\
    \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots   \\
    0 &amp; 0 &amp; \cdots &amp; \alpha_{nn}
  \end{pmatrix}.\]</span></p>
</div>
<p>Recall that an <em>algebraically closed field</em> is field in which <em>every</em> polynomial is a product of linear factors. For example, <span class="math inline">\(\mathbb{C}\)</span> is an algebraically closed field, by the Fundamental Theorem of Algebra.</p>
<p>Here’s why upper triangular matrices are a good thing.</p>
<div class="prop">
<p><span id="prop-upper-triangular-good" label="prop-upper-triangular-good"></span> Let <span class="math inline">\(V\)</span> be an <span class="math inline">\(n\)</span>-dimensional vector space over an algebraically closed field <span class="math inline">\(F\)</span>, let <span class="math inline">\(T : V\to V\)</span> be a linear transformation, and let <span class="math inline">\(A = [\alpha_{ij}]\)</span> be an upper triangular matrix for <span class="math inline">\(T\)</span> with respect to some basis for <span class="math inline">\(V\)</span>. Then the following hold:</p>
<ol type="1">
<li><p><span class="math inline">\(\det(A) = \alpha_{11}\alpha_{22} \cdots \alpha_{nn}\)</span>;</p></li>
<li><p><span class="math inline">\(A\)</span> is invertible if and only if none of <span class="math inline">\(\alpha_{11}, \alpha_{22},  \ldots, \alpha_{nn}\)</span> is <span class="math inline">\(0\)</span>;</p></li>
<li><p>the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(\alpha_{11}, \alpha_{22}, \ldots,  \alpha_{nn}\)</span>;</p></li>
<li><p>the characteristic polynomial <span class="math inline">\(c_A(x)\)</span> is <span class="math inline">\((x - \alpha_{11})(x -  \alpha_{22})\cdots(x - \alpha_{nn})\)</span>.</p></li>
</ol>
<p>In other words, upper triangular matrices are <span class="sans-serif">nice</span>.</p>
</div>
<div class="lemma">
<p><span id="lemma-upper-triangular" label="lemma-upper-triangular"></span> Let <span class="math inline">\(T: V \to V\)</span> be a linear transformation and let <span class="math inline">\(\mathscr{B} = \{v_1,  v_2, \ldots, v_n\}\)</span> be a basis for <span class="math inline">\(V\)</span>. Then <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)\)</span> is upper triangular if and only if <span class="math inline">\(T(v_i) \in \operatorname{Span}(v_1, \ldots, v_i)\)</span> for all <span class="math inline">\(i \in  \{1, 2, \ldots, n\}\)</span>.</p>
</div>
<div class="proof">
<p><em>Proof.</em> It is straightforward to verify this directly from the definitions of upper triangular matrices and the definition of <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)\)</span>. ◻</p>
</div>
<div class="thm">
<p><span id="cor:quotient:triangular" label="cor:quotient:triangular"></span> Let <span class="math inline">\(V\)</span> be an <span class="math inline">\(n\)</span>-dimensional vector space over an algebraically closed field <span class="math inline">\(F\)</span>, and let <span class="math inline">\(T : V\to V\)</span> be a linear transformation. Then there is a basis <span class="math inline">\(\mathscr{B}\)</span> of <span class="math inline">\(V\)</span> so that <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span> is upper triangular.</p>
</div>
<div class="proof">
<p><em>Proof.</em> We proceed by induction on the dimension <span class="math inline">\(n\)</span> of <span class="math inline">\(V\)</span>. Every <span class="math inline">\(1 \times 1\)</span> matrix is upper triangular, and so the result holds for <span class="math inline">\(n = 1\)</span>.</p>
<p>Suppose that <span class="math inline">\(n &gt; 1\)</span> and that the conclusion of the theorem holds for all vector spaces over <span class="math inline">\(F\)</span> with dimension strictly less than <span class="math inline">\(n\)</span>. Since <span class="math inline">\(F\)</span> is algebraically closed, it follows from <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#prop-eigenvalue-root">Proposition 8.2.2</a> that <span class="math inline">\(T\)</span> has an eigenvalue <span class="math inline">\(\lambda \in F\)</span>. We define <span class="math display">\[W = \operatorname{im} (T - \lambda \operatorname{id}),\]</span> where <span class="math inline">\(\operatorname{id}: V\to V\)</span> is the identity linear transformation. If <span class="math inline">\(v\in  V\setminus \{\vec{0}\}\)</span> is the eigenvector with eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(v\in\ker(T - \lambda \operatorname{id})\)</span> (the eigenspace of <span class="math inline">\(\lambda\)</span>). Hence <span class="math inline">\(\dim\ker(T  - \lambda \operatorname{id}) &gt; 0\)</span> and so, by the Rank-Nullity Theorem (<a href="https://jdbm.me/mt3501-lnotes/04-linear-transf/#thm-rank-nullity">Theorem 4.1.7</a>), <span class="math display">\[\dim W = \dim \operatorname{im} (T - \lambda \operatorname{id}) = \dim V - \dim \ker(T - \lambda \operatorname{id})  &lt; \dim V.\]</span> To apply the inductive hypothesis, we require a linear transformation from <span class="math inline">\(W\)</span> to <span class="math inline">\(W\)</span>. If <span class="math inline">\(w\in W\)</span>, then <span class="math inline">\(T(w) = (T- \lambda \operatorname{id})(w) + \lambda w\)</span> and since <span class="math inline">\((T- \lambda \operatorname{id})(w)  \in \operatorname{im} (T- \lambda \operatorname{id}) = W\)</span> and <span class="math inline">\(\lambda w \in W\)</span>, it follows that <span class="math inline">\(T(w)  \in W\)</span>. If we define <span class="math inline">\(S : W \to W\)</span> by <span class="math inline">\(S(w) = T(w)\)</span> for all <span class="math inline">\(w\in W\)</span>, then <span class="math inline">\(S\)</span> is a linear transformation.</p>
<p>Hence, by the inductive hypothesis, there exists a basis <span class="math inline">\(\mathscr{C} = \{w_1, w_2, \ldots, w_m\}\)</span> for <span class="math inline">\(W\)</span> such that <span class="math inline">\(\operatorname{Mat}_{\mathscr{C}, \mathscr{C}}(S)\)</span> is upper triangular. In particular, by <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#lemma-upper-triangular">Lemma 8.3.3</a>, <span class="math inline">\(T(w_i) = S(w_i) \in  \operatorname{Span}(w_1, \ldots, w_i)\)</span> for all <span class="math inline">\(w_i\in \mathscr{C}\)</span>. If we extend <span class="math inline">\(\mathscr{C}\)</span> to a basis <span class="math inline">\(\mathscr{B} = \{v_1 = w_1, v_2 = w_2, \ldots, v_m = w_m,  v_{m + 1}, \ldots, v_n\}\)</span> for <span class="math inline">\(V\)</span>, then, for every <span class="math inline">\(i\in \{m + 1, \ldots,  n\}\)</span>, <span class="math display">\[T(v_i) = (T - \lambda \operatorname{id})(v_i) + \lambda v_i.\]</span> Certainly, <span class="math display">\[(T - \lambda \operatorname{id})(v_i) \in \operatorname{im} (T - \lambda\operatorname{id}) = W = \operatorname{Span}(w_1,
    \ldots, w_m) = \operatorname{Span}(v_1, \ldots, v_m)\]</span> and so <span class="math display">\[T(v_i) = (T - \lambda \operatorname{id})(v_i) + \lambda v_i 
    \in  
    \operatorname{Span}(v_1, \ldots, v_m, v_i)
    \subseteq 
    \operatorname{Span}(v_1, \ldots, v_m, v_{m + 1}, \ldots, v_i).\]</span> Hence, again by <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#lemma-upper-triangular">Lemma 8.3.3</a>, <span class="math inline">\(\operatorname{Mat}_{\mathscr{B},\mathscr{B}}(T)\)</span> is upper triangular. ◻</p>
</div>
<p>Note that although <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#cor:quotient:triangular">Theorem 8.3.4</a> states that every linear transformation from a vector space over an algebraically closed field to itself has an upper triangular matrix, neither the statement of the result nor the proof is constructive. In other words, neither the theorem nor its proof says how to find such an upper triangular matrix (at least not in an easy to compute way).</p>
<h2 id="the-cayleyhamilton-theorem">The Cayley–Hamilton Theorem</h2>
<p>The subject of this section is the following theorem.</p>
<div class="thm">
<p><span id="thm-cayley-hamilton" label="thm-cayley-hamilton"></span> Let <span class="math inline">\(V\)</span> be a finite-dimensional vector space and let <span class="math inline">\(T : V \to V\)</span> be a linear transformation. If <span class="math inline">\(c_{T}(x)\)</span> is the characteristic polynomial of <span class="math inline">\(T\)</span>, then <span class="math inline">\(c_{T}(T) = \vec{0}_{\L(V, V)}\)</span>, where <span class="math inline">\(\vec{0}_{\L(V, V)}\)</span> is the linear transformation of <span class="math inline">\(V\)</span> mapping every vector <span class="math inline">\(v\in V\)</span> to <span class="math inline">\(\vec{0}_V\)</span>.</p>
</div>
<p>We showed in Problem 5 on the Problem Sheet for Section 8 (Eigen etc) that the matrix of the linear transformation <span class="math inline">\(c_T(T)\)</span> equals <span class="math inline">\(c_T(A)\)</span> where <span class="math inline">\(A\)</span> is the matrix of <span class="math inline">\(T\)</span>. It follows that <span class="math inline">\(c_T(A)\)</span> is the zero matrix.</p>
<div class="exampjupyter">
<p>We showed in <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#ex-no-eigenvalues">Example 8.2.6</a> that the characteristic polynomial of <span class="math inline">\(T: \mathbb{R} ^ 2 \to \mathbb{R} ^ 2\)</span> with matrix <span class="math display">\[A =
    \begin{pmatrix}
      1 &amp; -2 \\
      2 &amp; 0
    \end{pmatrix}\]</span> is <span class="math display">\[c_A(x) = x ^ 2 - x + 4.\]</span> The Cayley-Hamilton Theorem asserts that <span class="math display">\[c_A(A) = A ^ 2 - A + 4I\]</span> is the <span class="math inline">\(2\times 2\)</span> zero matrix, where <span class="math inline">\(I\)</span> is the <span class="math inline">\(2\times 2\)</span> identity matrix. You can verify that this is true by direct computation.</p>
</div>
<div class="proof">
<p><em>Proof of <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#thm-cayley-hamilton">Theorem 8.4.1</a>.</em> We will give a proof of the special case of <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#thm-cayley-hamilton">Theorem 8.4.1</a> when the field <span class="math inline">\(F\)</span> over which the vector space <span class="math inline">\(V\)</span> is defined is algebraically closed<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>By <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#cor:quotient:triangular">Theorem 8.3.4</a>, there exists a basis <span class="math inline">\(\mathscr{B} = \{w_1, w_2,  \ldots, w_n\}\)</span> for <span class="math inline">\(V\)</span> such that the matrix <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)\)</span> of <span class="math inline">\(T\)</span> with respect to <span class="math inline">\(\mathscr{B}\)</span> is upper triangular. We will denote <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)\)</span> by <span class="math inline">\(A =  [\alpha_{ij}]\)</span>. If <span class="math inline">\(\dim V = n\)</span>, then, by <a href="https://jdbm.me/mt3501-lnotes/08-eigen-stuff/#prop-upper-triangular-good">Proposition 8.3.2</a>(4), <span class="math display">\[c_T(x) = (x - \alpha_{11})(x - \alpha_{22})\cdots (x - \alpha_{nn}).\]</span> If we substitute <span class="math inline">\(T\)</span> for <span class="math inline">\(x\)</span> in the previous equation, we obtain <span class="math display">\[c_T(T) = (T - \alpha_{11}\operatorname{id})(T - \alpha_{22}\operatorname{id})\cdots (T - \alpha_{nn}\operatorname{id}).\]</span> So, <span class="math inline">\(c_T(T)\)</span> is itself a linear transformation from <span class="math inline">\(V\)</span> to <span class="math inline">\(V\)</span> (as a composite of the linear transformations <span class="math inline">\(T - \alpha_{ii}\operatorname{id}\)</span>).</p>
<p>We define <span class="math inline">\(W_0 = \{\vec{0}\}\)</span> and <span class="math inline">\(W_k = \operatorname{Span}(w_1, \ldots,  w_k)\)</span> for <span class="math inline">\(k \in \{1, \ldots, n\}\)</span>. Note that <span class="math display">\[W_0 \subseteq W_1 \subseteq W_2\subseteq \cdots \subseteq W_n.\]</span> Since <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)\)</span> is upper triangular, and by the definition of <span class="math inline">\(\operatorname{Mat}_{\mathscr{B}, \mathscr{B}}(T)\)</span>, it follows that <span class="math display">\[\label{eq-proof-of-ch}
    T(w_i) = \alpha_{1i}w_1 + \alpha_{2i}w_2 + \cdots + \alpha_{ii}w_i \in
    \operatorname{Span}(w_1, w_2, \ldots, w_i) = W_i\]</span> for every <span class="math inline">\(i\in \{1, \ldots, n\}\)</span>.</p>
<p>We will show that <span class="math inline">\((T- \alpha_{kk} \operatorname{id})(W_k) \subseteq W_{k - 1}\)</span> for all <span class="math inline">\(k\)</span>.</p>
<p>Suppose that <span class="math inline">\(w_k\in \mathscr{B}\)</span> is arbitrary. Then <span class="math display">\[\begin{aligned}
    (T- \alpha_{kk}\operatorname{id})(w_k) &amp;= T(w_k) - \alpha_{kk}w_k \\
                             &amp;= \alpha_{1k}w_1  + \cdots + \alpha_{k-1\ k}w_{k -
                             1} + \alpha_{kk}w_k - \alpha_{kk}w_k\\
                             &amp;= \alpha_{1k}w_1  + \cdots + \alpha_{k-1\ k}w_{k
                             - 1} \in \operatorname{Span}(w_1, \ldots, w_{k -1}) = W_{k -1}.
  \end{aligned}\]</span> On the other hand, if <span class="math inline">\(j \in \{1, \ldots, k - 1\}\)</span>, then <span class="math display">\[(T- \alpha_{kk}\operatorname{id})(w_j) = T(w_j) - \alpha_{kk}w_j 
                             = \alpha_{1j}w_1  + \cdots + \alpha_{jj}w_j -
                             \alpha_{kk}w_j
                             \in \operatorname{Span}(w_1, \ldots, w_{j}) = W_{j} \subseteq
                             W_{k -1}.\]</span> Hence <span class="math inline">\((T- \alpha_{kk}\operatorname{id})(w_j)\in W_{k - 1}\)</span> for all <span class="math inline">\(j\in \{1, \ldots, k\}\)</span> and for all <span class="math inline">\(k \in \{1, \ldots,  n\}\)</span>.</p>
<p>If <span class="math inline">\(w\in W_k\)</span> is arbitrary, then there exist <span class="math inline">\(\beta_1,  \beta_2,\ldots, \beta_k\in F\)</span> such that <span class="math display">\[w = \beta_1w_1 + \beta_2w_2 + \cdots + \beta_k w_k.\]</span> This implies that <span class="math display">\[(T - \alpha_{kk}\operatorname{id})(w)
    = 
    \underbrace{\beta_1(T - \alpha_{kk}\operatorname{id})(w_1)}_{\in W_{k-1}} + 
    \underbrace{\beta_2(T - \alpha_{kk}\operatorname{id})(w_2)}_{\in W_{k-1}} 
    + \cdots + 
    \underbrace{\beta_k(T - \alpha_{kk}\operatorname{id})(w_k)}_{\in W_{k - 1}}
    \in W_{k - 1}.\]</span> Since <span class="math inline">\(k\)</span> and <span class="math inline">\(w\in W_k\)</span> were arbitrary, it follows that <span class="math display">\[(T - \alpha_{kk}\operatorname{id})(W_k) \subseteq W_{k - 1}\quad \text{for all }k \in 
    \{1, \ldots, n\}.\]</span> But <span class="math inline">\(W_n = \operatorname{Span}(w_1, \ldots, w_n) = V\)</span>, and so <span class="math display">\[\begin{aligned}
    c_T(T)(V)  &amp; = &amp; 
    (T - \alpha_{11}\operatorname{id})(T - \alpha_{22}\operatorname{id})\cdots (T - \alpha_{nn}\operatorname{id})(V) \\
    &amp; = &amp; 
    (T - \alpha_{11}\operatorname{id})(T - \alpha_{22}\operatorname{id})\cdots (T - \alpha_{nn}\operatorname{id})(W_n) \\
    &amp; \subseteq &amp; 
    (T - \alpha_{11}\operatorname{id})(T - \alpha_{22}\operatorname{id})\cdots (T - \alpha_{n-1\, n-1
    }\operatorname{id})(W_{n - 1}) \\
    &amp; \vdots &amp;  \\
    &amp; \subseteq &amp; (T - \alpha_{11}\operatorname{id})(W_1) \\
    &amp; \subseteq &amp; W_0 = \{\vec{0}_{V}\}.
  \end{aligned}\]</span> Therefore <span class="math inline">\(c_T(T)(v) = \vec{0}_V\)</span> for every <span class="math inline">\(v\in V\)</span> and so <span class="math inline">\(c_T(T) = \vec{0}_{\L(V, V)}\)</span>, as required. ◻</p>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The general case can be reduced to the case when <span class="math inline">\(F\)</span> is algebraically closed via the observation that for any matrix <span class="math inline">\(A\)</span> with entries in <span class="math inline">\(F\)</span>, <span class="math inline">\(c_A(x)\)</span> does not change if we interpret <span class="math inline">\(A\)</span> as having entries in the algebraic closure <span class="math inline">\(\bar F\)</span>. For example, interpret a real matrix as having complex entries. Interested and Galois-theoretically inclined students can chase down the details.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>







<p><a href="#">Back to top</a></p>
</body>
</html>
